{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c002d364-479a-478d-a38c-6aac7a041c60",
   "metadata": {},
   "source": [
    "# Single-Molecule Localization Microscopy (SMLM) 2D Digits 123 and TOL letters datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccccb40-d04e-4c6f-b4b5-5dc30c226822",
   "metadata": {},
   "source": [
    "## Download data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014215b7-e2a6-4ea8-a6e1-7a90a8954f25",
   "metadata": {},
   "source": [
    "1. Downloaded dataset from https://data.4tu.nl/articles/dataset/Single-Molecule_Localization_Microscopy_SMLM_2D_Digits_123_and_TOL_letters_datasets/14074091/1\n",
    "2. Unzipped dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ac5a32-9556-4e10-9a71-edce2eb9b3cc",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3c4ad90-f51c-4405-bec3-4972082c090c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "import json\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6153673-087a-4267-8687-b2d97ada5e4f",
   "metadata": {},
   "source": [
    "## Convert .mat to .parquet and add in class information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bc9d5be-061c-4f24-aa6e-8c75fe5ebb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_label_map = {\n",
    "    0: \"one\",\n",
    "    1: \"two\",\n",
    "    2: \"three\",\n",
    "    3: \"T\",\n",
    "    4: \"O\",\n",
    "    5: \"L\",\n",
    "}\n",
    "gt_label_map_lookup = {\n",
    "    \"one\": 0,\n",
    "    \"two\": 1,\n",
    "    \"three\":2,\n",
    "    \"T\":3,\n",
    "    \"O\":4,\n",
    "    \"L\":5,\n",
    "}\n",
    "gt_label_map = json.dumps(gt_label_map).encode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ed8b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "####  testing\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## Create folder for preprocessed files\n",
    "output_file_path = \"./data/\"\n",
    "if not os.path.exists(output_file_path):\n",
    "    os.makedirs(output_file_path)\n",
    "\n",
    "## Load in separated letters\n",
    "folders = [\n",
    "          \"Letters/TOL_Imaged_Separately/Particles_L\",\n",
    "          \"Letters/TOL_Imaged_Separately/Particles_O\",\n",
    "          \"Letters/TOL_Imaged_Separately/Particles_T\",\n",
    "          \"Digits/Imaged_Separately/Particles_1\",\n",
    "          \"Digits/Imaged_Separately/Particles_2\",\n",
    "          \"Digits/Imaged_Separately/Particles_3\"\n",
    "          ]\n",
    "labels = [\n",
    "    \"L\", \n",
    "    \"O\", \n",
    "    \"T\", \n",
    "    \"one\", \n",
    "    \"two\", \n",
    "    \"three\"\n",
    "    ]\n",
    "for index, f in enumerate(folders):\n",
    "    assert len(labels) == len(folders)\n",
    "    mat = loadmat(f)\n",
    "    print(mat)\n",
    "    num_particles = len(mat[\"Particles\"][0,:])\n",
    "    for i in range(num_particles):\n",
    "        mat_pos = mat[\"Particles\"][0,:][i][0][0][0]\n",
    "        mat_sigma = mat[\"Particles\"][0,:][i][0][0][1]\n",
    "        x = pa.array(mat_pos[:,0])\n",
    "        y = pa.array(mat_pos[:,1])\n",
    "        sigma = pa.array(mat_sigma[:,0])\n",
    "        channel = pa.array([0]*len(mat_pos[:,0]))\n",
    "        if int(str(gt_label_map_lookup[labels[index]])) == 1:\n",
    "            plt.scatter(x, y)\n",
    "            plt.show()\n",
    "            input(\"stop\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35ba3c27-b4f8-440c-ac72-fbc418468ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create folder for preprocessed files\n",
    "output_file_path = \"./data/\"\n",
    "if not os.path.exists(output_file_path):\n",
    "    os.makedirs(output_file_path)\n",
    "\n",
    "## Load in separated letters\n",
    "folders = [\"Letters/TOL_Imaged_Separately/Particles_L\",\n",
    "          \"Letters/TOL_Imaged_Separately/Particles_O\",\n",
    "          \"Letters/TOL_Imaged_Separately/Particles_T\",\n",
    "          \"Digits/Imaged_Separately/Particles_1\",\n",
    "          \"Digits/Imaged_Separately/Particles_2\",\n",
    "          \"Digits/Imaged_Separately/Particles_3\"]\n",
    "labels = [\"L\", \"O\", \"T\", \"one\", \"two\", \"three\"]\n",
    "for index, f in enumerate(folders):\n",
    "    assert len(labels) == len(folders)\n",
    "    mat = loadmat(f)\n",
    "    num_particles = len(mat[\"Particles\"][0,:])\n",
    "    for i in range(num_particles):\n",
    "        mat_pos = mat[\"Particles\"][0,:][i][0][0][0]\n",
    "        mat_sigma = mat[\"Particles\"][0,:][i][0][0][1]\n",
    "        x = pa.array(mat_pos[:,0])\n",
    "        y = pa.array(mat_pos[:,1])\n",
    "        sigma = pa.array(mat_sigma[:,0])\n",
    "        channel = pa.array([0]*len(mat_pos[:,0]))\n",
    "        table = pa.table([x,y,sigma, channel], names=[\"x\",\"y\",\"sigma\", \"channel\"])\n",
    "        meta_data = {\n",
    "            \"gt_label\": str(gt_label_map_lookup[labels[index]]),\n",
    "            \"gt_label_map\": gt_label_map,\n",
    "        }\n",
    "        # merge existing with new meta data and save\n",
    "        old_metadata = table.schema.metadata\n",
    "        merged_metadata = {**meta_data, **(old_metadata or {})}\n",
    "        table = table.replace_schema_metadata(merged_metadata)\n",
    "        file = f\"{labels[index]}_{i}.parquet\"\n",
    "        save_loc =  f'{output_file_path}/{file}'\n",
    "        pq.write_table(table, save_loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d134dd26-a844-4926-b4dd-a7de61f837b1",
   "metadata": {},
   "source": [
    "## Split data into training (90%) and test set (10%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c049718-9cf9-4478-9327-cfd49c546a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_train_path = \"./data/train\"\n",
    "output_test_path = \"./data/test\"\n",
    "if not os.path.exists(output_train_path):\n",
    "    os.makedirs(output_train_path)\n",
    "if not os.path.exists(output_test_path):\n",
    "    os.makedirs(output_test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a32dea-8098-4006-bbd2-1c0840d13ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(\"./data/\")\n",
    "print(\"Length of files\", len(files))\n",
    "\n",
    "# split 90%/10% whole dataset\n",
    "train_length  = int(0.9*len(files)) - 4 # the -4 is custom added to ensure divisble by num classes\n",
    "test_length = len(files) - train_length\n",
    "assert test_length % 6 == 0\n",
    "\n",
    "# get per class split\n",
    "class_test_length = int(test_length/6)\n",
    "\n",
    "def split(file_id):\n",
    "    sub_files = np.array([f for f in files if f.startswith(file_id)])\n",
    "    np.random.shuffle(sub_files)\n",
    "    test = sub_files[0:class_test_length]\n",
    "    train = sub_files[class_test_length:]\n",
    "    return train, test\n",
    "\n",
    "# split files\n",
    "L_train, L_test = split(\"L_\")\n",
    "O_train, O_test = split(\"O_\")\n",
    "T_train, T_test = split(\"T_\")\n",
    "one_train, one_test = split(\"one_\")\n",
    "two_train, two_test = split(\"two_\")\n",
    "three_train, three_test = split(\"three_\")\n",
    "\n",
    "# combine files\n",
    "train = np.concatenate((L_train,O_train, T_train, one_train, two_train, three_train))\n",
    "test = np.concatenate((L_test, O_test, T_test, one_test, two_test, three_test))\n",
    "\n",
    "print(len(train))\n",
    "print(len(test))\n",
    "\n",
    "# move files\n",
    "for file in train:\n",
    "    src_path = os.path.join(\"./data/\", file)\n",
    "    dest_path = os.path.join(\"./data/train/\", file)\n",
    "    shutil.move(src_path, dest_path)\n",
    "    \n",
    "# move files\n",
    "for file in test:\n",
    "    src_path = os.path.join(\"./data/\", file)\n",
    "    dest_path = os.path.join(\"./data/test/\", file)\n",
    "    shutil.move(src_path, dest_path)\n",
    "\n",
    "print(\"Train files \", len(os.listdir(\"./data/train\")))\n",
    "print(\"Test files \", len(os.listdir(\"./data/test\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909961d8-d5b0-479e-95bd-db3173a058c1",
   "metadata": {},
   "source": [
    "## Dataset breakdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b123fc-8a27-47a3-955d-19b86076d100",
   "metadata": {},
   "source": [
    "Total dataset: 14,351\n",
    "\n",
    "Train: 12911 - [921 x L, 751 x T, 320 x O, 3915 x one, 4703 x two, 2301 x three]\n",
    "\n",
    "Test: 1440 - [240 of each]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a230a79-d1c8-42c5-82c7-2a008b808bbb",
   "metadata": {},
   "source": [
    "## Add in Grid class "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a80b580",
   "metadata": {},
   "source": [
    "Initially we missed there was a class called particle grid therefore need to add this in \n",
    "\n",
    "We want to maintain the test set and just add to it - to avoid data leakage & We will do the same for the train set \n",
    "- We need to go through all the existing train/test files and change the gt_label_map to include a label for 6: \"grid\"\n",
    "\n",
    "Then we need to process the grid images and\n",
    "1. Take 240 images and move to the test set\n",
    "2. Put the remaining in train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "95170b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_label_map = {\n",
    "    0: \"one\",\n",
    "    1: \"two\",\n",
    "    2: \"three\",\n",
    "    3: \"T\",\n",
    "    4: \"O\",\n",
    "    5: \"L\",\n",
    "    6: \"grid\",\n",
    "}\n",
    "gt_label_map = json.dumps(gt_label_map).encode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3596cc9b",
   "metadata": {},
   "source": [
    "### Change gt label map for all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "82f5e0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder = \"./data/train\"\n",
    "test_folder = \"./data/test\"\n",
    "\n",
    "train_files = os.listdir(train_folder)\n",
    "test_files = os.listdir(test_folder)\n",
    "\n",
    "for file in train_files:\n",
    "    path = os.path.join(train_folder, file)\n",
    "    table = pq.read_table(path)\n",
    "\n",
    "    meta_data = {\n",
    "        \"gt_label_map\": gt_label_map,\n",
    "    }\n",
    "    \n",
    "    # merge existing with new meta data and save\n",
    "    old_metadata = table.schema.metadata\n",
    "    old_metadata[b\"gt_label_map\"] = gt_label_map\n",
    "    table = table.replace_schema_metadata(old_metadata)\n",
    "    pq.write_table(table, path)\n",
    "\n",
    "for file in test_files:\n",
    "    path = os.path.join(test_folder, file)\n",
    "    table = pq.read_table(path)\n",
    "\n",
    "    meta_data = {\n",
    "        \"gt_label_map\": gt_label_map,\n",
    "    }\n",
    "    \n",
    "    # merge existing with new meta data and save\n",
    "    old_metadata = table.schema.metadata\n",
    "    old_metadata[b\"gt_label_map\"] = gt_label_map\n",
    "    table = table.replace_schema_metadata(old_metadata)\n",
    "    pq.write_table(table, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388192b7",
   "metadata": {},
   "source": [
    "### Process grid images\n",
    "RUN FROM HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344ba743",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create folder for preprocessed files\n",
    "output_file_path = \"./data_grid/\"\n",
    "\n",
    "if not os.path.exists(output_file_path):\n",
    "    os.makedirs(output_file_path)\n",
    "\n",
    "## Load in separated letters\n",
    "f = \"Digits/Imaged_Separately/Particles_Grid\"\n",
    "\n",
    "mat = loadmat(f)\n",
    "\n",
    "num_particles = len(mat[\"Particles\"][0,:])\n",
    "print(num_particles)\n",
    "for i in range(num_particles):\n",
    "    mat_pos = mat[\"Particles\"][0,:][i][0][0][0]\n",
    "    mat_sigma = mat[\"Particles\"][0,:][i][0][0][1]\n",
    "    x = pa.array(mat_pos[:,0])\n",
    "    y = pa.array(mat_pos[:,1])\n",
    "    sigma = pa.array(mat_sigma[:,0])\n",
    "    channel = pa.array([0]*len(mat_pos[:,0]))\n",
    "    table = pa.table([x,y,sigma, channel], names=[\"x\",\"y\",\"sigma\", \"channel\"])\n",
    "    \n",
    "    meta_data = {\n",
    "        \"gt_label\": str(6),\n",
    "        \"gt_label_map\": gt_label_map,\n",
    "    }\n",
    "\n",
    "    # merge existing with new meta data and save\n",
    "    old_metadata = table.schema.metadata\n",
    "    merged_metadata = {**meta_data, **(old_metadata or {})}\n",
    "    table = table.replace_schema_metadata(merged_metadata)\n",
    "    file = f\"grid_{i}.parquet\"\n",
    "    save_loc =  f'{output_file_path}/{file}'\n",
    "    pq.write_table(table, save_loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e69a81b",
   "metadata": {},
   "source": [
    "### Split grid images into train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82db2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(\"./data_grid/\")\n",
    "print(\"Length of files\", len(files))\n",
    "\n",
    "# split 90%/10% whole dataset\n",
    "test_length  = 240\n",
    "train_length = len(files) - test_length\n",
    "\n",
    "# split files\n",
    "np.random.shuffle(files)\n",
    "test_files = files[0:test_length]\n",
    "print(len(test_files))\n",
    "train_files = files[test_length:]\n",
    "print(len(train_files))\n",
    "\n",
    "# move files\n",
    "for file in train_files:\n",
    "    src_path = os.path.join(\"./data_grid/\", file)\n",
    "    dest_path = os.path.join(\"./data/train/\", file)\n",
    "    shutil.move(src_path, dest_path)\n",
    "    \n",
    "# move files\n",
    "for file in test_files:\n",
    "    src_path = os.path.join(\"./data_grid/\", file)\n",
    "    dest_path = os.path.join(\"./data/test/\", file)\n",
    "    shutil.move(src_path, dest_path)\n",
    "\n",
    "print(\"Train files \", len(os.listdir(\"./data/train\")))\n",
    "print(\"Test files \", len(os.listdir(\"./data/test\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adbcfaa",
   "metadata": {},
   "source": [
    "## Dataset breakdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93a5a02",
   "metadata": {},
   "source": [
    "Total dataset: 22,047\n",
    "\n",
    "Train: 20,367 - [921 x L, 751 x T, 320 x O, 3915 x one, 4703 x two, 2301 x three, 7456 x grid]\n",
    "\n",
    "Test: 1,680 - [240 of each]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c00089",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "locpix-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
