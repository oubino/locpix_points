{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import output_file, save\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "from locpix_points.data_loading import datastruc\n",
    "from locpix_points.scripts.visualise import visualise_torch_geometric, visualise_parquet, load_file\n",
    "from locpix_points.evaluation.featanalyse import (\n",
    "    explain,\n",
    "    generate_umap_embedding,\n",
    "    visualise_umap_embedding,\n",
    "    generate_pca_embedding,\n",
    "    visualise_pca_embedding,\n",
    "    visualise_explanation,\n",
    "    k_means_fn,\n",
    "    get_prediction,\n",
    "    subgraph_eval,\n",
    "    pgex_eval,\n",
    "    attention_eval,\n",
    "    test_ensemble_averaging,\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import umap\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_graph_path(project_directory, file_name, file_folder):\n",
    "    \"\"\"Visualise raw data\n",
    "    \n",
    "    Args:\n",
    "        project_directory (string): Location of project directory\n",
    "        file_name (string) : Name of file to image\n",
    "        file_folder (string) : Which folder the file is in\"\"\"\n",
    "    \n",
    "    train_file_map_path = os.path.join(project_directory, f\"{file_folder}/train/file_map.csv\")\n",
    "    val_file_map_path = os.path.join(project_directory, f\"{file_folder}/val/file_map.csv\")\n",
    "    test_file_map_path = os.path.join(project_directory, f\"{file_folder}/test/file_map.csv\")\n",
    "    \n",
    "    train_file_map = pd.read_csv(train_file_map_path)\n",
    "    val_file_map = pd.read_csv(val_file_map_path)\n",
    "    test_file_map = pd.read_csv(test_file_map_path)\n",
    "    \n",
    "    train_out = train_file_map[train_file_map[\"file_name\"] == file_name]\n",
    "    val_out = val_file_map[val_file_map[\"file_name\"] == file_name]\n",
    "    test_out = test_file_map[test_file_map[\"file_name\"] == file_name]\n",
    "    \n",
    "    if len(train_out) > 0:\n",
    "        folder = \"train\"\n",
    "        file_name = train_out[\"idx\"].values[0]\n",
    "    if len(val_out) > 0:\n",
    "        folder = \"val\"\n",
    "        file_name = val_out[\"idx\"].values[0]\n",
    "    if len(test_out) > 0:\n",
    "        folder = \"test\"\n",
    "        file_name = test_out[\"idx\"].values[0]\n",
    "    \n",
    "    return os.path.join(project_directory, f\"{file_folder}/{folder}/{file_name}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_directory = \"..\"\n",
    "# load config\n",
    "with open(os.path.join(project_directory, \"config/featanalyse_manual.yaml\"), \"r\") as ymlfile:\n",
    "    config_manual = yaml.safe_load(ymlfile)\n",
    "with open(os.path.join(project_directory, \"config/featanalyse_nn.yaml\"), \"r\") as ymlfile:\n",
    "    config_nn = yaml.safe_load(ymlfile)\n",
    "label_map = config_manual[\"label_map\"]\n",
    "assert label_map == config_nn[\"label_map\"]\n",
    "manual_features = config_manual[\"features\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test = True\n",
    "umap_n_neighbours = 20\n",
    "umap_min_dist = 0.5\n",
    "pca_n_components = 2\n",
    "device = 'cuda'\n",
    "n_repeats=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse the nn features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if final_test:  \n",
    "    test_df_nn_loc = os.path.join(project_directory, \"output/test_df_nn_loc.csv\")\n",
    "    test_df_nn_loc = pd.read_csv(test_df_nn_loc)\n",
    "\n",
    "    test_df_nn_cluster = os.path.join(project_directory, \"output/test_df_nn_cluster.csv\")\n",
    "    test_df_nn_cluster = pd.read_csv(test_df_nn_cluster)\n",
    "\n",
    "    test_df_nn_fov = os.path.join(project_directory, \"output/test_df_nn_fov.csv\")\n",
    "    test_df_nn_fov = pd.read_csv(test_df_nn_fov)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_umap_embedding_nn_loc_path = os.path.join(project_directory, \"output/test_umap_embedding_nn_loc.pkl\")\n",
    "test_umap_embedding_nn_cluster_path = os.path.join(project_directory, \"output/test_umap_embedding_nn_cluster.pkl\")\n",
    "test_umap_embedding_nn_fov_path = os.path.join(project_directory, \"output/test_umap_embedding_nn_fov.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"------ HANDCRAFTED FEATURES -------\")\n",
    "#with open(train_umap_embedding_nn_loc_path, \"rb\") as f:\n",
    "#        train_umap_embedding_nn_loc = pickle.load(f)\n",
    "#visualise_umap_embedding(train_umap_embedding_nn_loc, train_df_nn_loc, label_map)\n",
    "if final_test:\n",
    "    with open(test_umap_embedding_nn_loc_path, \"rb\") as f:\n",
    "        test_umap_embedding_nn_loc = pickle.load(f)\n",
    "    visualise_umap_embedding(test_umap_embedding_nn_loc, test_df_nn_loc, label_map, interactive=True)#save=True, save_name=\"clusternet_only_handcrafted_cluster_features_umap_nn_20_mindist_0.5\", project_directory=project_directory, point_size=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"------ CLUSTER ENCODER -------\")\n",
    "#with open(train_umap_embedding_nn_cluster_path, \"rb\") as f:\n",
    "#        train_umap_embedding_nn_cluster = pickle.load(f)\n",
    "#visualise_umap_embedding(train_umap_embedding_nn_cluster, train_df_nn_cluster, label_map)\n",
    "if final_test:\n",
    "    with open(test_umap_embedding_nn_cluster_path, \"rb\") as f:\n",
    "            test_umap_embedding_nn_cluster = pickle.load(f)\n",
    "    visualise_umap_embedding(test_umap_embedding_nn_cluster, test_df_nn_cluster, label_map, interactive=True)#, save=True, save_name=\"clusternet_only_nn_cluster_encoder_umap_nn_20_mindist_0.5\", project_directory=project_directory, point_size=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"------ FOV ENCODER -------\")\n",
    "#with open(train_umap_embedding_nn_fov_path, \"rb\") as f:\n",
    "#        train_umap_embedding_nn_fov = pickle.load(f)\n",
    "#visualise_umap_embedding(train_umap_embedding_nn_fov, train_df_nn_fov, label_map)\n",
    "if final_test:\n",
    "    with open(test_umap_embedding_nn_fov_path, \"rb\") as f:\n",
    "        test_umap_embedding_nn_fov = pickle.load(f)\n",
    "    plot = visualise_umap_embedding(test_umap_embedding_nn_fov, test_df_nn_fov, label_map, interactive=True)# save=True, save_name=\"clusternet_only_nn_fov_encoder_umap_nn_20_mindist_0.5\", project_directory=project_directory, point_size=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load in configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in gt_label_map\n",
    "metadata_path = os.path.join(project_directory, \"metadata.json\")\n",
    "with open(\n",
    "    metadata_path,\n",
    ") as file:\n",
    "    metadata = json.load(file)\n",
    "    # add time ran this script to metadata\n",
    "    gt_label_map = metadata[\"gt_label_map\"]\n",
    "\n",
    "gt_label_map = {int(key): val for key, val in gt_label_map.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_model = torch.load(os.path.join(project_directory, f\"output/cluster_model.pt\"))\n",
    "cluster_model.to(device)\n",
    "cluster_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_train_folder = os.path.join(project_directory, \"processed/featanalysis/train\")\n",
    "cluster_val_folder = os.path.join(project_directory, \"processed/featanalysis/val\")\n",
    "cluster_test_folder = os.path.join(project_directory, \"processed/featanalysis/test\")\n",
    "\n",
    "cluster_train_set = datastruc.ClusterDataset(\n",
    "    None,\n",
    "    cluster_train_folder,\n",
    "    label_level=None,\n",
    "    pre_filter=None,\n",
    "    save_on_gpu=None,\n",
    "    transform=None,\n",
    "    pre_transform=None,\n",
    "    fov_x=None,\n",
    "    fov_y=None,\n",
    ")\n",
    "\n",
    "cluster_val_set = datastruc.ClusterDataset(\n",
    "    None,\n",
    "    cluster_val_folder,\n",
    "    label_level=None,\n",
    "    pre_filter=None,\n",
    "    save_on_gpu=None,\n",
    "    transform=None,\n",
    "    pre_transform=None,\n",
    "    fov_x=None,\n",
    "    fov_y=None,\n",
    ")\n",
    "\n",
    "cluster_test_set = datastruc.ClusterDataset(\n",
    "    None,\n",
    "    cluster_test_folder,\n",
    "    label_level=None,\n",
    "    pre_filter=None,\n",
    "    save_on_gpu=None,\n",
    "    transform=None,\n",
    "    pre_transform=None,\n",
    "    fov_x=None,\n",
    "    fov_y=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify incorrectly predicted points in the UMAP for the test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "files = plot.renderers[0].data_source.data[\"file_name\"]\n",
    "files = [x.removesuffix('.parquet') for x in files]\n",
    "\n",
    "wrong_files = []\n",
    "for file in files:\n",
    "    file_name = file\n",
    "    x, pred = get_prediction(\n",
    "        file_name,\n",
    "        cluster_model, \n",
    "        cluster_train_set, \n",
    "        cluster_val_set, \n",
    "        cluster_test_set, \n",
    "        project_directory,\n",
    "        device, \n",
    "        gt_label_map)\n",
    "    if x.y.detach().item() != pred:\n",
    "        wrong_files.append(x.name)\n",
    "\n",
    "new_colors = [\"#000000\"]*len(files)\n",
    "for id, file in enumerate(files):\n",
    "    if file in wrong_files:\n",
    "        new_colors[id] = \"#FF0000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.renderers[0].data_source.data[\"new_colors\"] = new_colors\n",
    "plot.renderers[0].glyph.fill_color = 'new_colors'\n",
    "plot.renderers[0].glyph.line_color = 'new_colors'\n",
    "umap.plot.show(plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Publication figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1.anchored_artists import AnchoredSizeBar\n",
    "from torch_geometric.utils.convert import to_networkx\n",
    "import networkx as nx\n",
    "from networkx.drawing import draw_networkx, draw\n",
    "import numpy as np\n",
    "from matplotlib.colors import ListedColormap, Normalize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify points for each class furthest and closest to rest of class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = plot.renderers[0].data_source.data[\"x\"]\n",
    "y = plot.renderers[0].data_source.data[\"y\"]\n",
    "labels = plot.renderers[0].data_source.data[\"item\"]\n",
    "new_colors = plot.renderers[0].data_source.data[\"new_colors\"]\n",
    "file_name = plot.renderers[0].data_source.data[\"file_name\"]\n",
    "unique_labels = set(labels)\n",
    "df = pd.DataFrame({\"x\": x, \"y\": y, \"label\": labels, \"correct\": new_colors, \"name\": file_name})\n",
    "for label in unique_labels:\n",
    "    print(\"Label: \", label)\n",
    "    class_df = df[df[\"label\"] == label]\n",
    "    x_mean = np.mean(class_df[\"x\"])\n",
    "    y_mean = np.mean(class_df[\"y\"])\n",
    "    class_df[\"dist\"] = ((class_df[\"x\"]-x_mean)**2 + (class_df[\"y\"]-y_mean)**2)**0.5\n",
    "    min = class_df.loc[class_df[\"dist\"].idxmin()]\n",
    "    max = class_df.loc[class_df[\"dist\"].idxmax()]\n",
    "    print(\"--- Min ---\")\n",
    "    print(min)\n",
    "    print(\"--- Max ---\")\n",
    "    print(max)\n",
    "    print(\"---------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This gives files alternating closest then furthest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\"two_4524\", \"two_1407\", \"T_467\", \"T_33\", \"L_375\", \"L_825\", \"grid_7301\", \"grid_3676\", \"O_68\", \"O_292\", \"one_4017\", \"one_928\", \"three_2437\", \"three_2463\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Raw files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not final_test:\n",
    "    file_folder = \"preprocessed/gt_label\"\n",
    "else:\n",
    "    file_folder = \"preprocessed/test/gt_label\"\n",
    "fig, ax = plt.subplots(14,1,figsize=(20,80), sharex=True, sharey=True)\n",
    "for idx, file_name in enumerate(files):\n",
    "    file_path = os.path.join(project_directory, file_folder, file_name + \".parquet\")\n",
    "    df, unique_chans = load_file(file_path, \"x\", \"y\", None, \"channel\")\n",
    "    x = df[\"x\"].to_numpy()\n",
    "    y = df[\"y\"].to_numpy()\n",
    "    ax[idx].set_aspect('equal', adjustable='box')\n",
    "    ax[idx].scatter(y, x, s=1, c='k')\n",
    "    ax[idx].axis('off')\n",
    "    scalebar = AnchoredSizeBar(ax[idx].transData,\n",
    "                               0.1, '', 'lower left', \n",
    "                               pad=1,\n",
    "                               color='k',\n",
    "                               frameon=False,\n",
    "                               size_vertical=0.01)\n",
    "\n",
    "    ax[idx].add_artist(scalebar)\n",
    "output_path = os.path.join(project_directory, \"output\", \"combined\" + '_raw_s_1.svg') \n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "#plt.savefig(output_path, transparent=True, bbox_inches=\"tight\", pad_inches=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not final_test:\n",
    "    gt_file_folder = \"preprocessed/gt_label\"\n",
    "    feat_file_folder = \"preprocessed/featextract/locs\"\n",
    "else:\n",
    "    gt_file_folder = \"preprocessed/test/gt_label\"\n",
    "    feat_file_folder = \"preprocessed/test/featextract/locs\"\n",
    "\n",
    "fig, ax = plt.subplots(14,1,figsize=(20,80), sharex=True, sharey=True)\n",
    "colors_grey = ['0.8', (0.0, 1.0, 0.0), (0.9198330167772646, 0.00019544195496590255, 0.9023663764628042), (0.022826063681157582, 0.5658432009989469, 0.9292042754527637), (1.0, 0.5, 0.0), (0.2022271667963922, 0.004776515828955663, 0.892404204324589), (0.3303283202899151, 0.4608491026134133, 0.2941030733894585), (0.5, 1.0, 0.5), (0.7723074963983451, 0.0066115490293984225, 0.15243662980903372), (0.9136952591189091, 0.5104151769385785, 0.7797496184063708), (1.0, 1.0, 0.0), (0.0, 1.0, 1.0), (0.4996633088717094, 0.7906621743682507, 0.01563627319525085)]\n",
    "cmap_grey = ListedColormap(colors_grey)\n",
    "colors = [(0.0, 1.0, 0.0), (0.9198330167772646, 0.00019544195496590255, 0.9023663764628042), (0.022826063681157582, 0.5658432009989469, 0.9292042754527637), (1.0, 0.5, 0.0), (0.2022271667963922, 0.004776515828955663, 0.892404204324589), (0.3303283202899151, 0.4608491026134133, 0.2941030733894585), (0.5, 1.0, 0.5), (0.7723074963983451, 0.0066115490293984225, 0.15243662980903372), (0.9136952591189091, 0.5104151769385785, 0.7797496184063708), (1.0, 1.0, 0.0), (0.0, 1.0, 1.0), (0.4996633088717094, 0.7906621743682507, 0.01563627319525085)]\n",
    "cmap_no_grey = ListedColormap(colors)\n",
    "\n",
    "for idx, file_name in enumerate(files):\n",
    "    file_path = os.path.join(project_directory, gt_file_folder, file_name + \".parquet\")\n",
    "    df_gt = pl.read_parquet(file_path)\n",
    "    file_path = os.path.join(project_directory, feat_file_folder, file_name + \".parquet\")\n",
    "    df_feat = pl.read_parquet(file_path)\n",
    "    df = df_feat.join(df_gt, on=[\"x\", \"y\", \"channel\", \"frame\"], how = \"outer\")\n",
    "    df = df.with_columns(pl.col(\"clusterID\").fill_null(-1))\n",
    "    assert df[\"channel\"].unique().item() == 0\n",
    "    x = df[\"x\"].to_numpy()\n",
    "    y = df[\"y\"].to_numpy()\n",
    "    c = df[\"clusterID\"].to_numpy()\n",
    "    if np.min(c) == -1:\n",
    "        cmap = cmap_grey\n",
    "    else:\n",
    "        cmap = cmap_no_grey\n",
    "    ax[idx].set_aspect('equal', adjustable='box')\n",
    "    ax[idx].scatter(y, x, s=1, c=c, cmap=cmap)\n",
    "    ax[idx].axis('off')\n",
    "    scalebar = AnchoredSizeBar(ax[idx].transData,\n",
    "                            0.1, '', 'lower left', \n",
    "                            pad=1,\n",
    "                            color='k',\n",
    "                            frameon=False,\n",
    "                            size_vertical=0.01)\n",
    "\n",
    "    ax[idx].add_artist(scalebar)\n",
    "output_path = os.path.join(project_directory, \"output\", \"combined\" + '_clustered_s_1.svg') \n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "#plt.savefig(output_path, transparent=True, bbox_inches=\"tight\", pad_inches=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SubgraphX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subgraph_config = {\n",
    "    # number of iterations to get prediction\n",
    "    \"rollout\":  20, # 20\n",
    "    # number of atoms of leaf node in search tree\n",
    "    \"min_atoms\": 5,\n",
    "    # hyperparameter that encourages exploration\n",
    "    \"c_puct\": 10.0,\n",
    "    # number of atoms to expand when extend the child nodes in the search tree\n",
    "    \"expand_atoms\": 14,\n",
    "    # whether to expand the children nodes from high degreee to low degree when extend the child nodes in the search tree\n",
    "    \"high2low\": False,\n",
    "    # number of local radius to caclulate\n",
    "    \"local_radius\": 4,\n",
    "    # sampling time of montecarlo approxim\n",
    "    \"sample_num\": 100, # 100\n",
    "    # reward method\n",
    "    \"reward_method\": \"mc_l_shapley\",\n",
    "    # subgrpah building method\n",
    "    \"subgraph_building_method\": \"split\",\n",
    "    # maximum number of nodes to include in subgraph when generating explanation\n",
    "    \"max_nodes\": 8,\n",
    "    # number of classes\n",
    "    \"num_classes\": 7,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# visualise overlaid subgraph using matplotlib\n",
    "dataitems = torch.load(os.path.join(project_directory, \"output/subgraphx_dataitems_rollout_100.pt\"))\n",
    "node_imps = torch.load(os.path.join(project_directory, \"output/subgraphx_nodeimps_rollout_100.pt\"))\n",
    "\n",
    "if not final_test:\n",
    "    fold = config[\"fold\"]\n",
    "    file_folder = f\"processed/fold_{fold}\"\n",
    "else:\n",
    "    file_folder = \"processed\"\n",
    "\n",
    "fig, ax = plt.subplots(7,2,figsize=(10,40), sharex=True, sharey=True)\n",
    "for idx, file_name in enumerate(files):\n",
    "    \n",
    "    file_loc = find_graph_path(project_directory, file_name, file_folder)\n",
    "    # raw file\n",
    "    processed_file = torch.load(file_loc)\n",
    "    processed_file = processed_file.pos_dict['locs'].cpu().numpy()\n",
    "    x = processed_file[:,0]\n",
    "    y = processed_file[:,1]\n",
    "     # center points\n",
    "    x_mean = np.mean(x)\n",
    "    y_mean = np.mean(y)\n",
    "    x = x - x_mean \n",
    "    y = y - y_mean\n",
    "    ax[idx//2, idx%2].set_aspect('equal', adjustable='box')\n",
    "    ax[idx//2, idx%2].scatter(y, x, s=1, c='0.8')\n",
    "    #ax[idx].axis('off')\n",
    "    scalebar = AnchoredSizeBar(ax[idx//2, idx%2].transData,\n",
    "                               0.1, '', 'lower right', \n",
    "                               pad=1,\n",
    "                               color='k',\n",
    "                               frameon=False,\n",
    "                               size_vertical=0.01)\n",
    "\n",
    "    ax[idx//2, idx%2].add_artist(scalebar)\n",
    "    # graph\n",
    "    nx_g = to_networkx(dataitems[idx], to_undirected=True)\n",
    "    nx_g.remove_edges_from(nx.selfloop_edges(nx_g))\n",
    "    pos = dataitems[idx].pos.cpu().numpy()\n",
    "    node_color = np.where(node_imps[idx].cpu().numpy(), '#00FF00', 'k')\n",
    "    # center points\n",
    "    pos[:,0] = pos[:,0] - x_mean\n",
    "    pos[:,1] = pos[:,1] - y_mean\n",
    "    draw(nx_g, pos=np.flip(pos, axis= 1), ax=ax[idx//2, idx%2], node_color=node_color, node_size=50)\n",
    "output_path = os.path.join(project_directory, \"output\", \"combined\" + '_subgraphx_s_1.svg') \n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "#plt.savefig(output_path, transparent=True, bbox_inches=\"tight\", pad_inches=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualise clustering in processed graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# visualise overlaid subgraph using matplotlib\n",
    "\n",
    "dataitems = torch.load(os.path.join(project_directory, \"output/subgraphx_dataitems_rollout_100.pt\"))\n",
    "\n",
    "if not final_test:\n",
    "    fold = config[\"fold\"]\n",
    "    file_folder = f\"processed/fold_{fold}\"\n",
    "else:\n",
    "    file_folder = \"processed\"\n",
    "\n",
    "colors_grey = ['0.8', (0.0, 1.0, 0.0), (0.9198330167772646, 0.00019544195496590255, 0.9023663764628042), (0.022826063681157582, 0.5658432009989469, 0.9292042754527637), (1.0, 0.5, 0.0), (0.2022271667963922, 0.004776515828955663, 0.892404204324589), (0.3303283202899151, 0.4608491026134133, 0.2941030733894585), (0.5, 1.0, 0.5), (0.7723074963983451, 0.0066115490293984225, 0.15243662980903372), (0.9136952591189091, 0.5104151769385785, 0.7797496184063708), (1.0, 1.0, 0.0), (0.0, 1.0, 1.0), (0.4996633088717094, 0.7906621743682507, 0.01563627319525085)]\n",
    "cmap_grey = ListedColormap(colors_grey)\n",
    "colors = [(0.0, 1.0, 0.0), (0.9198330167772646, 0.00019544195496590255, 0.9023663764628042), (0.022826063681157582, 0.5658432009989469, 0.9292042754527637), (1.0, 0.5, 0.0), (0.2022271667963922, 0.004776515828955663, 0.892404204324589), (0.3303283202899151, 0.4608491026134133, 0.2941030733894585), (0.5, 1.0, 0.5), (0.7723074963983451, 0.0066115490293984225, 0.15243662980903372), (0.9136952591189091, 0.5104151769385785, 0.7797496184063708), (1.0, 1.0, 0.0), (0.0, 1.0, 1.0), (0.4996633088717094, 0.7906621743682507, 0.01563627319525085)]\n",
    "cmap_no_grey = ListedColormap(colors)\n",
    "\n",
    "fig, ax = plt.subplots(14,1,figsize=(20,80), sharex=True, sharey=True)\n",
    "for idx, file_name in enumerate(files):\n",
    "    \n",
    "    file_loc = find_graph_path(project_directory, file_name, file_folder)\n",
    "    # raw file\n",
    "    processed_file = torch.load(file_loc)\n",
    "    c = processed_file.edge_index_dict[\"locs\", \"in\", \"clusters\"][1].cpu().numpy()\n",
    "    processed_file = processed_file.pos_dict['locs'].cpu().numpy()\n",
    "    x = processed_file[:,0]\n",
    "    y = processed_file[:,1]\n",
    "     # center points\n",
    "    x_mean = np.mean(x)\n",
    "    y_mean = np.mean(y)\n",
    "    x = x - x_mean \n",
    "    y = y - y_mean\n",
    "    ax[idx].set_aspect('equal', adjustable='box')\n",
    "    \n",
    "    if np.min(c) == -1:\n",
    "        cmap = cmap_grey\n",
    "    else:\n",
    "        cmap = cmap_no_grey\n",
    "    ax[idx].scatter(y, x, s=1, c=c, cmap=cmap)\n",
    "    ax[idx].axis('off')\n",
    "    scalebar = AnchoredSizeBar(ax[idx].transData,\n",
    "                               0.1, '', 'lower right', \n",
    "                               pad=1,\n",
    "                               color='k',\n",
    "                               frameon=False,\n",
    "                               size_vertical=0.01)\n",
    "\n",
    "    ax[idx].add_artist(scalebar)\n",
    "   \n",
    "output_path = os.path.join(project_directory, \"output/processed_clusters.svg\") \n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "#plt.savefig(output_path, transparent=True, bbox_inches=\"tight\", pad_inches=0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
