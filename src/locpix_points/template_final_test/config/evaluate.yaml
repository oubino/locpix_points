# whether to load the data directly from the gpu
load_data_from_gpu: False

# whether making node or whole graph predictions
label_level: graph # Options: [node, graph]

# choice of model
model: [CHANGE] # Options: See below

# parameters for the network
[CHANGE]:
  [CHANGE]

# number of classes in the dataset
num_classes: 2

# whether to evaluate on the gpu
eval_on_gpu: True

# ----- Choice of models ------

# model: locclusternet
# locclusternet:
#
#   # +++++ PointNet parameters +++++
#   local_channels: [[2, 32, 32, 64],[130, 64, 64, 128]]
#   global_channels: [[64, 128, 128],[128, 256, 256]]
#   global_sa_channels: [258, 128, 128, 128]
#   final_channels: [128, 64, 32, 8]
#   # Ratio of points to sample from the point cloud
#   ratio: 1.0
#   # For each point we sample nearest neighbours up to radius and nearest neighbours (k)
#   radius: 1.0
#   k: 4
#
#   # +++++ ClusterNet parameters +++++
#   # Dropout for each layer during evaluate should be 0.0
#   dropout: 0.0 # CAREFUL CHANGING!
#   conv_type: gin # Options: [gin, transformer]
#   # number of final output channels
#   OutputChannels: 2
#
#   # --- if conv_type == gin ------------
#   ClusterEncoderChannels: [[8, 16, 16],[16,20,24],[24,28,32]]
#   # ------------------------------------
#
#   # --- if conv_type == transformer ----
#   ClusterEncoderOutChannels: [16, 24, 32]
#   # Number of multihead attention layers
#   heads: 1
#   # If False multi-head attentions are averaged rather than being concatenated
#   concat: True
#   # See https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.TransformerConv.html for difference
#   # when beta is True
#   beta: False
#   # ------------------------------------
#
# model: locclusternettransformer
# locclusternettransformer:
#
#   # +++++ PointTransformer parameters +++++
#   # Ratio of points to sample from the point cloud
#   ratio: 1.0
#   # For each point we sample nearest neighbours up to nearest neighbours (k)
#   k: 4
#   # Number of channels input to first MLP
#   in_channels: 0
#   # Number of channels out of the final layer
#   out_channels: 8
#   # Channels for the transformer/transition down blocks
#   dim_model: [16, 32, 64]
#   pos_nn_layers: 64
#   attn_nn_layers: 64
#   # Hidden channels for final MLP
#   output_mlp_layers: 128
#
#   # +++++ ClusterNet parameters +++++
#   # Dropout for each layer during evaluate should be 0.0
#   dropout: 0.0 # CAREFUL CHANGING!
#   conv_type: gin # Options: [gin, transformer]
#   # number of final output channels
#   OutputChannels: 2
#
#   # --- if conv_type == gin ------------
#   ClusterEncoderChannels: [[8, 16, 16],[16,20,24],[24,28,32]]
#   # ------------------------------------
#
#   # --- if conv_type == transformer ----
#   ClusterEncoderOutChannels: [16, 24, 32]
#   # Number of multihead attention layers
#   heads: 1
#   # If False multi-head attentions are averaged rather than being concatenated
#   concat: True
#   # See https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.TransformerConv.html for difference
#   # when beta is True
#   beta: False
#   # ------------------------------------
#
# model: clusternet
# clusternet:
#
#   # Dropout for each layer during evaluate should be 0.0
#   dropout: 0.0 # CAREFUL CHANGING!
#
#   # +++++ ClusterNet parameters +++++
#   conv_type: gin # Options: [gin, transformer]
#   # number of final output channels
#   OutputChannels: 2
#
#   # --- if conv_type == gin ------------
#   ClusterEncoderChannels: [[8, 16, 16],[16,20,24],[24,28,32]]
#   # ------------------------------------
#
#   # --- if conv_type == transformer ----
#   ClusterEncoderOutChannels: [16, 24, 32]
#   # Number of multihead attention layers
#   heads: 1
#   # If False multi-head attentions are averaged rather than being concatenated
#   concat: True
#   # See https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.TransformerConv.html for difference
#   # when beta is True
#   beta: False
#   # ------------------------------------
#
# model: clustermlp
# clustermlp:
#   # Dropout for each layer during evaluate should be 0.0
#   dropout: 0.0 # CAREFUL CHANGING!
#
#   # Channels for MLP applied to clusters
#   channels: [8, 16, 24, 32, 2]
#
# model: locnetonly_pointnet
# locnetonly_pointnet:
#   # +++++ PointNet parameters +++++
#   local_channels: [[2, 32, 32, 64],[130, 64, 64, 128]]
#   global_channels: [[64, 128, 128],[128, 256, 256]]
#   global_sa_channels: [258, 128, 128, 128]
#   final_channels: [128, 64, 32, 2]
#   # Ratio of points to sample from the point cloud
#   ratio: 1.0
#   # For each point we sample nearest neighbours up to radius and nearest neighbours (k)
#   radius: 1.0
#   k: 4
#
# model: locnetonly_pointtransformer
# locnetonly_pointtransformer:
#   # +++++ PointTransformer parameters +++++
#   # Ratio of points to sample from the point cloud
#   ratio: 1.0
#   # For each point we sample nearest neighbours up to nearest neighbours (k)
#   k: 4
#   # Number of channels input to first MLP
#   in_channels: 0
#   # Number of channels out of the final layer
#   out_channels: 2
#   # Channels for the transformer/transition down blocks
#   dim_model: [16, 32, 64]
#   pos_nn_layers: 64
#   attn_nn_layers: 64
#   # Hidden channels for final MLP
#   output_mlp_layers: 128
