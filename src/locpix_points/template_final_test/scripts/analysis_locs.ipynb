{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from locpix_points.scripts.visualise import visualise_torch_geometric, visualise_parquet, load_file\n",
    "from locpix_points.evaluate.locanalyse import(\n",
    "    analyse_locs,\n",
    "    explain,\n",
    ")\n",
    "from mpl_toolkits.axes_grid1.anchored_artists import AnchoredSizeBar\n",
    "from torch_geometric.utils.convert import to_networkx\n",
    "import networkx as nx\n",
    "from networkx.drawing import draw_networkx, draw\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import pandas as pd\n",
    "from torch_geometric.data import Data\n",
    "import torch\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_graph_path(project_directory, file_name, file_folder):\n",
    "    \"\"\"Visualise raw data\n",
    "    \n",
    "    Args:\n",
    "        project_directory (string): Location of project directory\n",
    "        file_name (string) : Name of file to image\n",
    "        file_folder (string) : Which folder the file is in\"\"\"\n",
    "    \n",
    "    train_file_map_path = os.path.join(project_directory, f\"{file_folder}/train/file_map.csv\")\n",
    "    val_file_map_path = os.path.join(project_directory, f\"{file_folder}/val/file_map.csv\")\n",
    "    test_file_map_path = os.path.join(project_directory, f\"{file_folder}/test/file_map.csv\")\n",
    "    \n",
    "    train_file_map = pd.read_csv(train_file_map_path)\n",
    "    val_file_map = pd.read_csv(val_file_map_path)\n",
    "    test_file_map = pd.read_csv(test_file_map_path)\n",
    "    \n",
    "    train_out = train_file_map[train_file_map[\"file_name\"] == file_name]\n",
    "    val_out = val_file_map[val_file_map[\"file_name\"] == file_name]\n",
    "    test_out = test_file_map[test_file_map[\"file_name\"] == file_name]\n",
    "    \n",
    "    if len(train_out) > 0:\n",
    "        folder = \"train\"\n",
    "        file_name = train_out[\"idx\"].values[0]\n",
    "    if len(val_out) > 0:\n",
    "        folder = \"val\"\n",
    "        file_name = val_out[\"idx\"].values[0]\n",
    "    if len(test_out) > 0:\n",
    "        folder = \"test\"\n",
    "        file_name = test_out[\"idx\"].values[0]\n",
    "    \n",
    "    return os.path.join(project_directory, f\"{file_folder}/{folder}/{file_name}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_directory = \"..\"\n",
    "final_test = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load config\n",
    "with open(os.path.join(project_directory, \"config/locanalyse.yaml\"), \"r\") as ymlfile:\n",
    "    config = yaml.safe_load(ymlfile)\n",
    "label_map = config[\"label_map\"]\n",
    "\n",
    "# load in gt_label_map\n",
    "metadata_path = os.path.join(project_directory, \"metadata.json\")\n",
    "with open(\n",
    "    metadata_path,\n",
    ") as file:\n",
    "    metadata = json.load(file)\n",
    "    # add time ran this script to metadata\n",
    "    gt_label_map = metadata[\"gt_label_map\"]\n",
    "\n",
    "assert {str(val):key for key, val in label_map.items()} == gt_label_map\n",
    "gt_label_map = {int(key): val for key, val in gt_label_map.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph structure explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get item to evaluate on\n",
    "file_name = \"three_1004\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualise raw file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not final_test:\n",
    "    file_folder = \"preprocessed/gt_label\"\n",
    "else:\n",
    "    file_folder = \"preprocessed/test/gt_label\"\n",
    "file_path = os.path.join(project_directory, file_folder, file_name + \".parquet\")\n",
    "print(file_path)\n",
    "visualise_parquet(file_path, 'y', 'x', None, 'channel', {0: \"channel_0\", 1: \"channel_1\", 2: \"channel_2\", 3: \"channel_3\"}, cmap=['k'], spheres=True, sphere_size=0.004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not final_test:\n",
    "    file_folder = \"preprocessed/gt_label\"\n",
    "else:\n",
    "    file_folder = \"preprocessed/test/gt_label\"\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(10,10), sharex=True, sharey=True)\n",
    "file_path = os.path.join(project_directory, file_folder, file_name + \".parquet\")\n",
    "df, unique_chans = load_file(file_path, \"x\", \"y\", None, \"channel\")\n",
    "x = df[\"x\"].to_numpy()\n",
    "y = df[\"y\"].to_numpy()\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "ax.scatter(y, x, s=10, c='k')\n",
    "ax.axis('off')\n",
    "scalebar = AnchoredSizeBar(ax.transData,\n",
    "                            0.1, '', 'lower left', \n",
    "                            pad=1,\n",
    "                            color='k',\n",
    "                            frameon=False,\n",
    "                            size_vertical=0.01)\n",
    "\n",
    "ax.add_artist(scalebar)\n",
    "output_path = os.path.join(project_directory, \"output\", \"combined\" + '_raw_s_10.svg') \n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "#plt.savefig(output_path, transparent=True, bbox_inches=\"tight\", pad_inches=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, train_map, test_set, test_map, model, model_type, config, device = analyse_locs(project_directory, config, final_test, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SubgraphX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "subgraph_config = {\n",
    "    # number of iterations to get prediction\n",
    "    \"rollout\": 20, \n",
    "    # number of atoms of leaf node in search tree\n",
    "    \"min_atoms\": 5,\n",
    "    # hyperparameter that encourages exploration\n",
    "    \"c_puct\": 10.0,\n",
    "    # number of atoms to expand when extend the child nodes in the search tree\n",
    "    \"expand_atoms\": 14,\n",
    "    # whether to expand the children nodes from high degreee to low degree when extend the child nodes in the search tree\n",
    "    \"high2low\": False,\n",
    "    # number of local radius to caclulate\n",
    "    \"local_radius\": 4,\n",
    "    # sampling time of montecarlo approxim\n",
    "    \"sample_num\": 100, \n",
    "    # reward method\n",
    "    \"reward_method\": \"mc_l_shapley\",\n",
    "    # subgrpah building method\n",
    "    \"subgraph_building_method\": \"split\",\n",
    "    # maximum number of nodes to include in subgraph when generating explanation\n",
    "    \"max_nodes\": 15,\n",
    "    # number of classes\n",
    "    \"num_classes\": 7,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # ---- subgraphx -----\n",
    "output = explain([file_name], train_map, train_set, model, model_type, config, device, type='subgraphx', subgraph_config=subgraph_config, intermediate=True)    \n",
    "subgraph, complement, data, node_imp = output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise overlaid subgraph using matplotlib\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(20,20), sharex=True, sharey=True)\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "scalebar = AnchoredSizeBar(ax.transData,\n",
    "                            0.1, '', 'lower right', \n",
    "                            pad=1,\n",
    "                            color='k',\n",
    "                            frameon=False,\n",
    "                            size_vertical=0.01)\n",
    "\n",
    "ax.add_artist(scalebar)\n",
    "# graph\n",
    "nx_g = to_networkx(data, to_undirected=True)\n",
    "nx_g.remove_edges_from(nx.selfloop_edges(nx_g))\n",
    "pos = data.pos.cpu().numpy()\n",
    "node_color = np.where(node_imp.cpu().numpy(), 'r', 'b')\n",
    "\n",
    "draw(nx_g, pos=np.flip(pos, axis= 1), ax=ax, node_color=node_color, node_size=50)\n",
    "output_path = os.path.join(project_directory, \"output\", \"combined\" + '_subgraphx_s_10.svg') \n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.savefig(output_path, transparent=True, bbox_inches=\"tight\", pad_inches=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- attention -----\n",
    "output = explain([file_name], train_map, train_set, model, model_type, config, device, type='attention')\n",
    "positions, edge_indices, alphas = output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas_ = []\n",
    "# threshold the attention values\n",
    "for alpha in alphas:\n",
    "    alphas_.append(torch.where(alpha > 0.0, 1.0, 0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise overlaid subgraph using matplotlib\n",
    "\n",
    "remove_self_loops_and_neg_edges = False\n",
    "\n",
    "if not final_test:\n",
    "    fold = config[\"fold\"]\n",
    "    file_folder = f\"processed/fold_{fold}\"\n",
    "else:\n",
    "    file_folder = \"processed\"\n",
    "\n",
    "colors = [(0, 0, 0), (1, 0, 0)] # first color is black, last is red\n",
    "cm = LinearSegmentedColormap.from_list(\"br\", colors)\n",
    "\n",
    "fig, ax = plt.subplots(3,1,figsize=(20,20), sharex=True, sharey=True)\n",
    "    \n",
    "# raw file\n",
    "for idx, position in enumerate(positions):\n",
    "\n",
    "    ax[idx].set_aspect('equal', adjustable='box')\n",
    "    ax[idx].axis('off')\n",
    "    scalebar = AnchoredSizeBar(ax[idx].transData,\n",
    "                                0.1, '', 'lower right', \n",
    "                                pad=1,\n",
    "                                color='k',\n",
    "                                frameon=False,\n",
    "                                size_vertical=0.01)\n",
    "\n",
    "    ax[idx].add_artist(scalebar)\n",
    "    # graph\n",
    "    dataitem = Data(x=None, edge_index=edge_indices[idx], pos=positions[idx])\n",
    "    nx_g = to_networkx(dataitem)\n",
    "    pos = dataitem.pos.cpu().numpy()\n",
    "    edge_color = cm(alphas_[idx].cpu())\n",
    "    if remove_self_loops_and_neg_edges:\n",
    "        neg_edges = np.argwhere(alphas_[idx].cpu().numpy() == 0.0)\n",
    "        neg_edges = np.array([e for e in nx_g.edges])[neg_edges[:,0]]\n",
    "        neg_edges = [tuple(val) for val in neg_edges]\n",
    "        edges = list(nx_g.edges)\n",
    "        remove_indices = [i for i, item in enumerate(edges) if item in neg_edges]\n",
    "        edge_color = np.delete(edge_color, remove_indices, axis=0)\n",
    "        nx_g.remove_edges_from(neg_edges)\n",
    "    self_loops = list(nx.selfloop_edges(nx_g))\n",
    "    edges = list(nx_g.edges)\n",
    "    remove_indices = [i for i, item in enumerate(edges) if item in self_loops]\n",
    "    edge_color = np.delete(edge_color, remove_indices, axis=0)\n",
    "    nx_g.remove_edges_from(nx.selfloop_edges(nx_g))\n",
    "    if remove_self_loops_and_neg_edges:\n",
    "        min = np.min(edge_color[:,0])\n",
    "        max = np.max(edge_color[:,0])\n",
    "        edge_color[:,0] = (edge_color[:,0] - min)/(max - min)\n",
    "    draw(nx_g, pos=np.flip(pos, axis= 1), ax=ax[idx], edge_color=edge_color, node_size=1, node_color='k')\n",
    "    output_path = os.path.join(project_directory, \"output\", \"combined\" + '_attention_all_edges_s_1.svg') \n",
    "    plt.subplots_adjust(wspace=0, hspace=0)\n",
    "\n",
    "    #plt.savefig(output_path, transparent=True, bbox_inches=\"tight\", pad_inches=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
