{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results given on the data when in the test folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import output_file, save\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "from locpix_points.data_loading import datastruc\n",
    "from locpix_points.scripts.generate_features import main as extract_features\n",
    "from locpix_points.scripts.visualise import visualise_torch_geometric, visualise_parquet, load_file\n",
    "from locpix_points.evaluation.featanalyse import (\n",
    "    explain,\n",
    "    generate_umap_embedding,\n",
    "    visualise_umap_embedding,\n",
    "    generate_pca_embedding,\n",
    "    visualise_pca_embedding,\n",
    "    visualise_explanation,\n",
    "    k_means_fn,\n",
    "    get_prediction,\n",
    "    subgraph_eval,\n",
    "    pgex_eval,\n",
    "    attention_eval,\n",
    "    test_ensemble_averaging,\n",
    "    struc_analysis_prep,\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import umap\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_graph_path(project_directory, file_name, file_folder):\n",
    "    \"\"\"Visualise raw data\n",
    "    \n",
    "    Args:\n",
    "        project_directory (string): Location of project directory\n",
    "        file_name (string) : Name of file to image\n",
    "        file_folder (string) : Which folder the file is in\"\"\"\n",
    "    \n",
    "    train_file_map_path = os.path.join(project_directory, f\"{file_folder}/train/file_map.csv\")\n",
    "    val_file_map_path = os.path.join(project_directory, f\"{file_folder}/val/file_map.csv\")\n",
    "    test_file_map_path = os.path.join(project_directory, f\"{file_folder}/test/file_map.csv\")\n",
    "    \n",
    "    train_file_map = pd.read_csv(train_file_map_path)\n",
    "    val_file_map = pd.read_csv(val_file_map_path)\n",
    "    test_file_map = pd.read_csv(test_file_map_path)\n",
    "    \n",
    "    train_out = train_file_map[train_file_map[\"file_name\"] == file_name]\n",
    "    val_out = val_file_map[val_file_map[\"file_name\"] == file_name]\n",
    "    test_out = test_file_map[test_file_map[\"file_name\"] == file_name]\n",
    "    \n",
    "    if len(train_out) > 0:\n",
    "        folder = \"train\"\n",
    "        file_name = train_out[\"idx\"].values[0]\n",
    "    if len(val_out) > 0:\n",
    "        folder = \"val\"\n",
    "        file_name = val_out[\"idx\"].values[0]\n",
    "    if len(test_out) > 0:\n",
    "        folder = \"test\"\n",
    "        file_name = test_out[\"idx\"].values[0]\n",
    "    \n",
    "    return os.path.join(project_directory, f\"{file_folder}/{folder}/{file_name}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_directory = \"..\"\n",
    "# load config\n",
    "config_loc = os.path.join(project_directory, \"config/featanalyse_nn.yaml\")\n",
    "with open(config_loc, \"r\") as ymlfile:\n",
    "    config_nn = yaml.safe_load(ymlfile)\n",
    "label_map = config_nn[\"label_map\"]\n",
    "file_map = \"../../../../../maps/linked_files.csv\"\n",
    "model_type = config_nn[\"model\"]\n",
    "model_name = config_nn[\"model_name\"]\n",
    "model_config = config_nn[model_type]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_n_neighbours = 20\n",
    "umap_min_dist = 0.5\n",
    "pca_n_components = 2\n",
    "device = 'cuda'\n",
    "n_repeats=25\n",
    "fold = 0\n",
    "interactive = False\n",
    "colour_by = \"response\" # options: [response, prediction, wt, wt_response, patient, correct]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [PATIENT] Load in patient information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linked_files = pd.read_csv(file_map)\n",
    "linked_files = linked_files.drop(columns=[\"file_name\"])\n",
    "linked_files = linked_files.rename(columns = {\"trial_no\": \"patient\"})\n",
    "linked_files = linked_files.drop_duplicates()\n",
    "\n",
    "# mutations column\n",
    "mutations = ['kras1213_sr', 'kras61_sr', 'kras146_sr', 'nras1213_sr', 'nras61_sr', 'braf_sr']\n",
    "linked_files['all_wt'] = linked_files[mutations].apply(lambda row: 'yes' if all(val in ['WT', 'W/T'] for val in row) else 'no', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <mark>Generate features for explanation</mark>\n",
    "\n",
    "CAREFUL WHETHER TO RUN AGAIN AS WILL CHANGE BELOW RESULTS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_features_q = input(\"Are you sure you want to extract features? (YES I AM SURE): \")\n",
    "if extract_features_q == \"YES I AM SURE\":\n",
    "    extract_features([\"-i\",project_directory,\"-c\",config_loc,\"-r\",f\"{n_repeats}\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_nn_loc = os.path.join(project_directory, \"output/train_locs.csv\")\n",
    "train_df_nn_loc = pd.read_csv(train_df_nn_loc)\n",
    "\n",
    "train_df_nn_cluster = os.path.join(project_directory, \"output/train_clusters.csv\")\n",
    "train_df_nn_cluster = pd.read_csv(train_df_nn_cluster)\n",
    "\n",
    "train_df_nn_fov = os.path.join(project_directory, \"output/train_fovs.csv\")\n",
    "train_df_nn_fov = pd.read_csv(train_df_nn_fov)\n",
    "\n",
    "val_df_nn_loc = os.path.join(project_directory, \"output/val_locs.csv\")\n",
    "val_df_nn_loc = pd.read_csv(val_df_nn_loc)\n",
    "\n",
    "val_df_nn_cluster = os.path.join(project_directory, \"output/val_clusters.csv\")\n",
    "val_df_nn_cluster = pd.read_csv(val_df_nn_cluster)\n",
    "\n",
    "val_df_nn_fov = os.path.join(project_directory, \"output/val_fovs.csv\")\n",
    "val_df_nn_fov = pd.read_csv(val_df_nn_fov)\n",
    "\n",
    "test_df_nn_loc = os.path.join(project_directory, \"output/test_locs.csv\")\n",
    "test_df_nn_loc = pd.read_csv(test_df_nn_loc)\n",
    "\n",
    "test_df_nn_cluster = os.path.join(project_directory, \"output/test_clusters.csv\")\n",
    "test_df_nn_cluster = pd.read_csv(test_df_nn_cluster)\n",
    "\n",
    "test_df_nn_fov = os.path.join(project_directory, \"output/test_fovs.csv\")\n",
    "test_df_nn_fov = pd.read_csv(test_df_nn_fov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_features(train_df, val_df, test_df, fold):\n",
    "\n",
    "    # get features present in the dataframe\n",
    "    not_features = [\"type\", \"file_name\", \"fold\", \"prediction\"]\n",
    "    features = [x for x in train_df.columns.to_list() if x not in not_features]\n",
    "\n",
    "    ############ WARNING ##############\n",
    "    # Be careful, if analysing neural net features\n",
    "    # Is this the number of features you expect\n",
    "    # Did this task use manual features as well\n",
    "    num_features = len(features)\n",
    "    print(\"Num features: \", num_features)\n",
    "\n",
    "    train_data_list = []\n",
    "    test_data_list = []\n",
    "\n",
    "    # -------------------------------------- # \n",
    "\n",
    "    # --- Note ----\n",
    "    ## If aggregate by folds, then we see the folds are separated in \n",
    "    ## UMAP space.\n",
    "    ## i.e. We DON'T SEE: patients or other groups close to each  \n",
    "    ## other in UMAP space irrelevant of the folds.\n",
    "    ## Instead, we see the folds separated.\n",
    "    \n",
    "    #for fold in range(5):\n",
    "\n",
    "    train_data = train_df[train_df[\"fold\"] == fold]\n",
    "    val_data = val_df[val_df[\"fold\"] == fold]\n",
    "    test_data = test_df[test_df[\"fold\"] == fold]\n",
    "            \n",
    "    # feature vector\n",
    "    train_data = train_data[features].values\n",
    "    val_data = val_data[features].values\n",
    "    test_data = test_data[features].values\n",
    "\n",
    "    # concatenate train and val\n",
    "    train_data = np.concatenate([train_data, val_data])\n",
    "\n",
    "    scaler = StandardScaler().fit(train_data)\n",
    "    X_train = scaler.transform(train_data)\n",
    "    X_test = scaler.transform(test_data)\n",
    "\n",
    "    train_data_list.append(X_train)\n",
    "    test_data_list.append(X_test)\n",
    "\n",
    "    # ------------------------------------- #\n",
    "\n",
    "    X_train = np.concatenate(train_data_list)\n",
    "    X_test = np.concatenate(test_data_list)\n",
    "        \n",
    "    return X_train, X_test\n",
    "\n",
    "_, X_test_nn_loc = prep_features(train_df_nn_loc, val_df_nn_loc, test_df_nn_loc, fold)\n",
    "_, X_test_nn_cluster = prep_features(train_df_nn_cluster, val_df_nn_cluster, test_df_nn_cluster, fold)\n",
    "_, X_test_nn_fov = prep_features(train_df_nn_fov, val_df_nn_fov, test_df_nn_fov, fold)\n",
    "\n",
    "test_df_nn_loc = test_df_nn_loc[test_df_nn_loc[\"fold\"] == fold]\n",
    "test_df_nn_cluster = test_df_nn_cluster[test_df_nn_cluster[\"fold\"] == fold]\n",
    "test_df_nn_fov = test_df_nn_fov[test_df_nn_fov[\"fold\"] == fold]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [PATIENT] Incorporate patient information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_nn_loc[\"patient\"] = test_df_nn_loc['file_name'].str.extract(r'_P(\\d+)')\n",
    "test_df_nn_loc = test_df_nn_loc.astype({'patient': 'int64'})\n",
    "test_df_nn_loc = test_df_nn_loc.merge(linked_files, on=\"patient\")\n",
    "\n",
    "test_df_nn_cluster[\"patient\"] = test_df_nn_cluster['file_name'].str.extract(r'_P(\\d+)')\n",
    "test_df_nn_cluster = test_df_nn_cluster.astype({'patient': 'int64'})\n",
    "test_df_nn_cluster = test_df_nn_cluster.merge(linked_files, on=\"patient\")\n",
    "\n",
    "test_df_nn_fov[\"patient\"] = test_df_nn_fov['file_name'].str.extract(r'_P(\\d+)')\n",
    "test_df_nn_fov = test_df_nn_fov.astype({'patient': 'int64'})\n",
    "test_df_nn_fov = test_df_nn_fov.merge(linked_files, on=\"patient\")\n",
    "\n",
    "test_df_nn_loc['wt_response'] = test_df_nn_loc.apply(lambda row: \n",
    "    \"wt_responder\" if row['type'] == 'any_response' and row['all_wt'] == 'yes' else \n",
    "    \"non_wt_responder\" if row['type'] == 'any_response' and row['all_wt'] == 'no' else \n",
    "    \"non_responder\" if row['type'] == 'no_response' else None, axis=1)\n",
    "\n",
    "test_df_nn_cluster['wt_response'] = test_df_nn_cluster.apply(lambda row: \n",
    "    \"wt_responder\" if row['type'] == 'any_response' and row['all_wt'] == 'yes' else \n",
    "    \"non_wt_responder\" if row['type'] == 'any_response' and row['all_wt'] == 'no' else \n",
    "    \"non_responder\" if row['type'] == 'no_response' else None, axis=1)\n",
    "\n",
    "test_df_nn_fov['wt_response'] = test_df_nn_fov.apply(lambda row: \n",
    "    \"wt_responder\" if row['type'] == 'any_response' and row['all_wt'] == 'yes' else \n",
    "    \"non_wt_responder\" if row['type'] == 'any_response' and row['all_wt'] == 'no' else \n",
    "    \"non_responder\" if row['type'] == 'no_response' else None, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_umap_embedding_nn_loc_path = os.path.join(project_directory, f\"output/test_umap_embedding_nn_loc_{fold}.pkl\")\n",
    "test_umap_embedding_nn_cluster_path = os.path.join(project_directory, f\"output/test_umap_embedding_nn_cluster_{fold}.pkl\")\n",
    "test_umap_embedding_nn_fov_path = os.path.join(project_directory, f\"output/test_umap_embedding_nn_fov_{fold}.pkl\")\n",
    "\n",
    "if not os.path.exists(test_umap_embedding_nn_loc_path):\n",
    "    test_umap_embedding_nn_loc = generate_umap_embedding(X_test_nn_loc, umap_min_dist, umap_n_neighbours)\n",
    "    with open(test_umap_embedding_nn_loc_path, \"wb\") as f:\n",
    "        pickle.dump(test_umap_embedding_nn_loc, f)\n",
    "    f.close()\n",
    "\n",
    "if not os.path.exists(test_umap_embedding_nn_cluster_path):\n",
    "    test_umap_embedding_nn_cluster = generate_umap_embedding(X_test_nn_cluster, umap_min_dist, umap_n_neighbours)\n",
    "    with open(test_umap_embedding_nn_cluster_path, \"wb\") as f:\n",
    "        pickle.dump(test_umap_embedding_nn_cluster, f)\n",
    "    f.close()\n",
    "\n",
    "if not os.path.exists(test_umap_embedding_nn_fov_path):\n",
    "    test_umap_embedding_nn_fov = generate_umap_embedding(X_test_nn_fov, umap_min_dist, umap_n_neighbours)\n",
    "    with open(test_umap_embedding_nn_fov_path, \"wb\") as f:\n",
    "        pickle.dump(test_umap_embedding_nn_fov, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"------ LOC ENCODER -------\")\n",
    "with open(test_umap_embedding_nn_loc_path, \"rb\") as f:\n",
    "    test_umap_embedding_nn_loc = pickle.load(f)\n",
    "visualise_umap_embedding(test_umap_embedding_nn_loc, test_df_nn_loc, label_map, interactive=interactive, colour_by=colour_by)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"------ CLUSTER ENCODER -------\")\n",
    "with open(test_umap_embedding_nn_cluster_path, \"rb\") as f:\n",
    "        test_umap_embedding_nn_cluster = pickle.load(f)\n",
    "visualise_umap_embedding(test_umap_embedding_nn_cluster, test_df_nn_cluster, label_map, interactive=interactive, colour_by=colour_by)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"------ FOV ENCODER -------\")\n",
    "with open(test_umap_embedding_nn_fov_path, \"rb\") as f:\n",
    "    test_umap_embedding_nn_fov = pickle.load(f)\n",
    "plot = visualise_umap_embedding(test_umap_embedding_nn_fov, test_df_nn_fov, label_map, interactive=interactive, colour_by=colour_by)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data for structure analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struc_analysis_prep(project_directory, fold, False, model_type, model_name, model_config, n_repeats, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get item to evaluate on\n",
    "file_name = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load in configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_label_map = {int(val): key for key,val in config_nn[\"label_map\"].items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualise raw file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_folder = \"preprocessed/gt_label\"\n",
    "file_path = os.path.join(project_directory, file_folder, file_name + \".parquet\")\n",
    "spheres = False\n",
    "sphere_size = 50\n",
    "visualise_parquet(file_path, 'x', 'y', None, 'channel', {0: \"channel_0\", 1: \"channel_1\", 2: \"channel_2\", 3: \"channel_3\"}, cmap=['k'], spheres=spheres, sphere_size=sphere_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualise file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_folder = f\"processed/fold_{fold}\"\n",
    "file_loc = find_graph_path(project_directory, file_name, file_folder)\n",
    "visualise_torch_geometric(file_loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load in cluster model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_model = torch.load(os.path.join(project_directory, f\"output/homogeneous_dataset/cluster_model.pt\"), weights_only=False)\n",
    "cluster_model.to(device)\n",
    "cluster_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_train_folder = os.path.join(project_directory, f\"output/homogeneous_dataset/fold_{fold}/train\")\n",
    "cluster_val_folder = os.path.join(project_directory, f\"output/homogeneous_dataset/fold_{fold}/val\")\n",
    "cluster_test_folder = os.path.join(project_directory, f\"output/homogeneous_dataset/fold_{fold}/test\")\n",
    "\n",
    "cluster_train_set = datastruc.ClusterDataset(\n",
    "    None,\n",
    "    cluster_train_folder,\n",
    "    label_level=None,\n",
    "    pre_filter=None,\n",
    "    save_on_gpu=None,\n",
    "    transform=None,\n",
    "    pre_transform=None,\n",
    "    fov_x=None,\n",
    "    fov_y=None,\n",
    ")\n",
    "\n",
    "cluster_val_set = datastruc.ClusterDataset(\n",
    "    None,\n",
    "    cluster_val_folder,\n",
    "    label_level=None,\n",
    "    pre_filter=None,\n",
    "    save_on_gpu=None,\n",
    "    transform=None,\n",
    "    pre_transform=None,\n",
    "    fov_x=None,\n",
    "    fov_y=None,\n",
    ")\n",
    "\n",
    "cluster_test_set = datastruc.ClusterDataset(\n",
    "    None,\n",
    "    cluster_test_folder,\n",
    "    label_level=None,\n",
    "    pre_filter=None,\n",
    "    save_on_gpu=None,\n",
    "    transform=None,\n",
    "    pre_transform=None,\n",
    "    fov_x=None,\n",
    "    fov_y=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get items to evaluate on "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_dataitem, prediction = get_prediction(\n",
    "    file_name,\n",
    "    cluster_model, \n",
    "    cluster_train_set, \n",
    "    cluster_val_set, \n",
    "    cluster_test_set, \n",
    "    project_directory,\n",
    "    cluster_train_folder,\n",
    "    cluster_val_folder,\n",
    "    cluster_test_folder,\n",
    "    device, \n",
    "    gt_label_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SubgraphX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subgraph_config = {\n",
    "    # number of iterations to get prediction\n",
    "    \"rollout\":  100, # 20\n",
    "    # number of atoms of leaf node in search tree\n",
    "    \"min_atoms\": 5,\n",
    "    # hyperparameter that encourages exploration\n",
    "    \"c_puct\": 10.0,\n",
    "    # number of atoms to expand when extend the child nodes in the search tree\n",
    "    \"expand_atoms\": 14,\n",
    "    # whether to expand the children nodes from high degreee to low degree when extend the child nodes in the search tree\n",
    "    \"high2low\": False,\n",
    "    # number of local radius to caclulate\n",
    "    \"local_radius\": 4,\n",
    "    # sampling time of montecarlo approxim\n",
    "    \"sample_num\": 100, # 100\n",
    "    # reward method\n",
    "    \"reward_method\": \"mc_l_shapley\",\n",
    "    # subgrpah building method\n",
    "    \"subgraph_building_method\": \"split\",\n",
    "    # maximum number of nodes to include in subgraph when generating explanation\n",
    "    \"max_nodes\": None,\n",
    "    # number of classes\n",
    "    \"num_classes\": None,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # ---- subgraphx -----\n",
    "_, _, cluster_dataitem, node_imp = subgraph_eval(cluster_model, device, subgraph_config, cluster_dataitem, prediction)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise overlaid subgraph\n",
    "\n",
    "file_folder = f\"processed/fold_{fold}\"\n",
    "file_loc = find_graph_path(project_directory, file_name, file_folder)\n",
    "\n",
    "visualise_explanation(\n",
    "    cluster_dataitem.pos,\n",
    "    cluster_dataitem.edge_index,\n",
    "    node_imp=node_imp.to(device),\n",
    "    edge_imp=None,\n",
    "    overlay=True,\n",
    "    file_loc=file_loc, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_config = {\n",
    "    #  scale: cluster\n",
    "    \"scale\": \"cluster\",\n",
    "    #  # how to combine attention scores across multiple attention heads\n",
    "    \"reduce\": \"max\",\n",
    "    # threshold to apply to edge mask for pyg explain\n",
    "    \"edge_mask_threshold\": 0.5,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- attention -----\n",
    "# use model - logprobs or clustermodel - raw\n",
    "attention_eval(cluster_model, attention_config, cluster_dataitem, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Legacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pca_embedding_nn_loc = generate_pca_embedding(X_train_nn_loc, pca_n_components)\n",
    "train_pca_embedding_nn_fov = generate_pca_embedding(X_train_nn_fov, pca_n_components)\n",
    "train_pca_embedding_nn_cluster = generate_pca_embedding(X_train_nn_cluster, pca_n_components)\n",
    "if final_test:\n",
    "    test_pca_embedding_nn_loc = generate_pca_embedding(X_test_nn_loc, pca_n_components)\n",
    "    test_pca_embedding_nn_fov = generate_pca_embedding(X_test_nn_fov, pca_n_components)\n",
    "    test_pca_embedding_nn_cluster = generate_pca_embedding(X_test_nn_cluster, pca_n_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"------ LOC ENCODER -------\")\n",
    "visualise_pca_embedding(train_pca_embedding_nn_loc, train_df_nn_loc, label_map)\n",
    "if final_test:\n",
    "    visualise_pca_embedding(test_pca_embedding_nn_loc, test_df_nn_loc, label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"------ CLUSTER ENCODER -------\")\n",
    "visualise_pca_embedding(train_pca_embedding_nn_cluster, train_df_nn_cluster, label_map)\n",
    "if final_test:\n",
    "    visualise_pca_embedding(test_pca_embedding_nn_cluster, test_df_nn_cluster, label_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"------ FOV ENCODER -------\")\n",
    "visualise_pca_embedding(train_pca_embedding_nn_fov, train_df_nn_fov, label_map)\n",
    "if final_test:\n",
    "    plot = visualise_pca_embedding(test_pca_embedding_nn_fov, test_df_nn_fov, label_map)\n",
    "    if False:\n",
    "        output_file_loc  = os.path.join(project_directory, \"output\", \"nn_fov.html\")\n",
    "        output_file(output_file_loc)\n",
    "        save(plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"----- LOC ------\")\n",
    "k_means_fn(X_train_nn_loc, train_df_nn_loc, label_map)\n",
    "if final_test:\n",
    "    k_means_fn(X_test_nn_loc, test_df_nn_loc, label_map)\n",
    "\n",
    "print(\"----- CLUSTER ------\")\n",
    "k_means_fn(X_train_nn_cluster, train_df_nn_cluster, label_map)\n",
    "if final_test:\n",
    "    k_means_fn(X_test_nn_cluster, test_df_nn_cluster, label_map)\n",
    "\n",
    "print(\"----- FOV ------\")\n",
    "k_means_fn(X_train_nn_fov, train_df_nn_fov, label_map)\n",
    "if final_test:\n",
    "    k_means_fn(X_test_nn_fov, test_df_nn_fov, label_map)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PgEx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgex_config = {\n",
    "    # threshold to apply to edge mask for pyg explain\n",
    "    \"edge_mask_threshold\": 0.5,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- pgexplainer ----\n",
    "pg_explainer = torch.load(os.path.join(project_directory, f\"output/pg_explainer.pt\"), weights_only=False) \n",
    "pgex_eval(cluster_model, pg_explainer, cluster_dataitem, device, pgex_config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
