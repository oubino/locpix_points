{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from locpix_points.data_loading import datastruc\n",
    "from locpix_points.scripts.visualise import visualise_torch_geometric\n",
    "from locpix_points.scripts.featanalyse import (\n",
    "    generate_umap_embedding,\n",
    "    visualise_umap_embedding,\n",
    "    generate_pca_embedding,\n",
    "    visualise_pca_embedding,\n",
    "    k_means_fn,\n",
    "    get_prediction,\n",
    "    subgraph_eval,\n",
    "    pgex_eval,\n",
    "    attention_eval,\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_file(project_directory, file_name, file_folder):\n",
    "    \"\"\"Visualise raw data\n",
    "    \n",
    "    Args:\n",
    "        project_directory (string): Location of project directory\n",
    "        file_name (string) : Name of file to image\n",
    "        file_folder (string) : Which folder the file is in\"\"\"\n",
    "    \n",
    "    train_file_map_path = os.path.join(project_directory, f\"{file_folder}/train/file_map.csv\")\n",
    "    val_file_map_path = os.path.join(project_directory, f\"{file_folder}/val/file_map.csv\")\n",
    "    test_file_map_path = os.path.join(project_directory, f\"{file_folder}/test/file_map.csv\")\n",
    "    \n",
    "    train_file_map = pd.read_csv(train_file_map_path)\n",
    "    val_file_map = pd.read_csv(val_file_map_path)\n",
    "    test_file_map = pd.read_csv(test_file_map_path)\n",
    "    \n",
    "    train_out = train_file_map[train_file_map[\"file_name\"] == file_name]\n",
    "    val_out = val_file_map[val_file_map[\"file_name\"] == file_name]\n",
    "    test_out = test_file_map[test_file_map[\"file_name\"] == file_name]\n",
    "    \n",
    "    if len(train_out) > 0:\n",
    "        folder = \"train\"\n",
    "        file_name = train_out[\"idx\"].values[0]\n",
    "    if len(val_out) > 0:\n",
    "        folder = \"val\"\n",
    "        file_name = val_out[\"idx\"].values[0]\n",
    "    if len(test_out) > 0:\n",
    "        folder = \"test\"\n",
    "        file_name = test_out[\"idx\"].values[0]\n",
    "    \n",
    "    file_loc = os.path.join(project_directory, f\"{file_folder}/{folder}/{file_name}.pt\")\n",
    "    visualise_torch_geometric(file_loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_directory = \"..\"\n",
    "# load config\n",
    "with open(os.path.join(project_directory, \"config/featanalyse_manual.yaml\"), \"r\") as ymlfile:\n",
    "    config_manual = yaml.safe_load(ymlfile)\n",
    "with open(os.path.join(project_directory, \"config/featanalyse_nn.yaml\"), \"r\") as ymlfile:\n",
    "    config_nn = yaml.safe_load(ymlfile)\n",
    "label_map = config_manual[\"label_map\"]\n",
    "assert label_map == config_nn[\"label_map\"]\n",
    "manual_features = config_manual[\"features\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test = False\n",
    "umap_n_neighbours = 20\n",
    "umap_min_dist = 0.5\n",
    "pca_n_components = 2\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse the manual features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = os.path.join(project_directory, \"output/train_df_manual.csv\")\n",
    "train_df = pl.read_csv(train_df)\n",
    "train_df_pd = train_df.to_pandas()\n",
    "\n",
    "if final_test:\n",
    "    test_df = os.path.join(project_directory, \"output/test_df_manual.csv\")\n",
    "    test_df = pl.read_csv(test_df)\n",
    "    test_df_pd = test_df.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare PCA vs Convex hull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(10,4))\n",
    "sns.scatterplot(data=train_df_pd, x = \"length_pca\", y=\"length_convex_hull\",s=5, ax=ax1)\n",
    "sns.scatterplot(data=train_df_pd, x = \"area_pca\", y=\"area_convex_hull\",s=5, ax=ax2)\n",
    "plt.show()\n",
    "\n",
    "if final_test:\n",
    "    fig, (ax1, ax2) = plt.subplots(1,2, figsize=(10,4))\n",
    "    sns.scatterplot(data=test_df_pd, x = \"length_pca\", y=\"length_convex_hull\",s=5, ax=ax1)\n",
    "    sns.scatterplot(data=test_df_pd, x = \"area_pca\", y=\"area_convex_hull\",s=5, ax=ax2)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cluster features boxplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of clusters per FOV, cluster type, ...\n",
    "train_cluster_counts = train_df[\"file_name\"].value_counts()\n",
    "print(\"Number of clusters per FOV:\", train_cluster_counts)\n",
    "# number of clusters in each class\n",
    "train_type_counts = train_df[\"type\"].value_counts()\n",
    "print(\"Number of clusters in each class:\", train_type_counts)\n",
    "\n",
    "# per fov features grouped by mean with std\n",
    "train_df_boxplot = train_df[manual_features + [\"type\", \"file_name\"]].to_pandas()\n",
    "\n",
    "fig, axs = plt.subplots(2,4, figsize=(20,8))\n",
    "assert len(manual_features) == 8\n",
    "counter = 0\n",
    "for feat in manual_features:\n",
    "    sns.boxplot(x='type', y=feat, data=train_df_boxplot, color='k', fill=False, flierprops=dict(marker='x', markersize=5), ax=axs[counter//4][counter%4])\n",
    "    counter += 1\n",
    "plt.show()\n",
    "\n",
    "\n",
    "if final_test:\n",
    "    # number of clusters per FOV, cluster type, ...\n",
    "    test_cluster_counts = test_df[\"file_name\"].value_counts()\n",
    "    print(\"Number of clusters per FOV:\", test_cluster_counts)\n",
    "    # number of clusters in each class\n",
    "    test_type_counts = test_df[\"type\"].value_counts()\n",
    "    print(\"Number of clusters in each class:\", test_type_counts)\n",
    "\n",
    "    # per fov features grouped by mean with std\n",
    "    test_df_boxplot = test_df[manual_features + [\"type\", \"file_name\"]].to_pandas()\n",
    "\n",
    "    fig, axs = plt.subplots(2,4, figsize=(20,8))\n",
    "    assert len(manual_features) == 8\n",
    "    counter = 0\n",
    "    for feat in manual_features:\n",
    "        sns.boxplot(x='type', y=feat, data=test_df_boxplot, color='k', fill=False, flierprops=dict(marker='x', markersize=5), ax=axs[counter//4][counter%4])\n",
    "        counter += 1\n",
    "    plt.show()\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph feature explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get features present in the dataframe\n",
    "not_features = [\"clusterID\", \"x_mean\", \"y_mean\", \"type\", \"file_name\"]\n",
    "features = [x for x in train_df.columns if x not in not_features]\n",
    "\n",
    "# now remove features not selected by user\n",
    "removed_features = [f for f in features if f not in manual_features]\n",
    "print(\"Removed features: \", removed_features)\n",
    "features = [f for f in features if f in manual_features]\n",
    "print(\"Features analysed: \", features)\n",
    "\n",
    "# feature vector\n",
    "train_data_feats = train_df_pd[features].values\n",
    "if final_test:\n",
    "    test_data_feats = test_df_pd[features].values\n",
    "\n",
    "num_features = len(train_data_feats[0])\n",
    "print(\"Num features: \", num_features)\n",
    "############ WARNING ##############\n",
    "# Be careful, if analysing neural net features\n",
    "# Is this the number of features you expect\n",
    "# Did this task use manual features as well\n",
    "\n",
    "scaler = StandardScaler().fit(train_data_feats)\n",
    "X_train = scaler.transform(train_data_feats)\n",
    "if final_test:\n",
    "    X_test = scaler.transform(test_data_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_umap_embedding = generate_umap_embedding(X_train, umap_min_dist, umap_n_neighbours)\n",
    "if final_test:\n",
    "    test_umap_embedding = generate_umap_embedding(X_test, umap_min_dist, umap_n_neighbours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "visualise_umap_embedding(train_umap_embedding, train_df_pd, label_map)\n",
    "if final_test:\n",
    "    visualise_umap_embedding(test_umap_embedding, test_df_pd, label_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pca_embedding = generate_pca_embedding(X_train, pca_n_components)\n",
    "if final_test:\n",
    "    test_pca_embedding = generate_pca_embedding(X_test, pca_n_components)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_pca_embedding(train_pca_embedding, train_df_pd, label_map)\n",
    "if final_test:\n",
    "    visualise_pca_embedding(test_pca_embedding, test_df_pd, label_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_means_fn(X_train, train_df_pd, label_map)\n",
    "if final_test:\n",
    "    k_means_fn(X_test, test_df_pd, label_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse the nn features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_nn_loc = os.path.join(project_directory, \"output/train_df_nn_loc.csv\")\n",
    "train_df_nn_loc = pd.read_csv(train_df_nn_loc)\n",
    "\n",
    "train_df_nn_cluster = os.path.join(project_directory, \"output/train_df_nn_cluster.csv\")\n",
    "train_df_nn_cluster = pd.read_csv(train_df_nn_cluster)\n",
    "\n",
    "train_df_nn_fov = os.path.join(project_directory, \"output/train_df_nn_fov.csv\")\n",
    "train_df_nn_fov = pd.read_csv(train_df_nn_fov)\n",
    "\n",
    "if final_test:  \n",
    "    test_df_nn_loc = os.path.join(project_directory, \"output/test_df_nn_loc.csv\")\n",
    "    test_df_nn_loc = pd.read_csv(test_df_nn_loc)\n",
    "\n",
    "    test_df_nn_cluster = os.path.join(project_directory, \"output/test_df_nn_cluster.csv\")\n",
    "    test_df_nn_cluster = pd.read_csv(test_df_nn_cluster)\n",
    "\n",
    "    test_df_nn_fov = os.path.join(project_directory, \"output/test_df_nn_fov.csv\")\n",
    "    test_df_nn_fov = pd.read_csv(test_df_nn_fov)\n",
    "\n",
    "else:\n",
    "    test_df_nn_loc = None\n",
    "    test_df_nn_cluster = None\n",
    "    test_df_nn_fov = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph feature explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_features(train_df, test_df):\n",
    "\n",
    "    # get features present in the dataframe\n",
    "    not_features = [\"type\", \"file_name\"]\n",
    "    features = [x for x in train_df.columns.to_list() if x not in not_features]\n",
    "\n",
    "    # feature vector\n",
    "    train_data_feats_nn = train_df[features].values\n",
    "    if final_test:\n",
    "        test_data_feats_nn = test_df[features].values\n",
    "\n",
    "    num_features = len(train_data_feats_nn[0])\n",
    "    print(\"Num features: \", num_features)\n",
    "    ############ WARNING ##############\n",
    "    # Be careful, if analysing neural net features\n",
    "    # Is this the number of features you expect\n",
    "    # Did this task use manual features as well\n",
    "\n",
    "    scaler = StandardScaler().fit(train_data_feats_nn)\n",
    "    X_train_nn = scaler.transform(train_data_feats_nn)\n",
    "    if final_test:\n",
    "        X_test_nn = scaler.transform(test_data_feats_nn)\n",
    "        \n",
    "        return X_train_nn, X_test_nn\n",
    "    else:\n",
    "        return X_train_nn, None\n",
    "\n",
    "X_train_nn_loc, X_test_nn_loc = prep_features(train_df_nn_loc, test_df_nn_loc)\n",
    "X_train_nn_cluster, X_test_nn_cluster = prep_features(train_df_nn_cluster, test_df_nn_cluster)\n",
    "X_train_nn_fov, X_test_nn_fov = prep_features(train_df_nn_fov, test_df_nn_fov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_umap_embedding_nn_loc = generate_umap_embedding(X_train_nn_loc, umap_min_dist, umap_n_neighbours)\n",
    "train_umap_embedding_nn_cluster = generate_umap_embedding(X_train_nn_cluster, umap_min_dist, umap_n_neighbours)\n",
    "train_umap_embedding_nn_fov = generate_umap_embedding(X_train_nn_fov, umap_min_dist, umap_n_neighbours)\n",
    "if final_test:\n",
    "    test_umap_embedding_nn_loc = generate_umap_embedding(X_test_nn_loc, umap_min_dist, umap_n_neighbours)\n",
    "    test_umap_embedding_nn_cluster = generate_umap_embedding(X_test_nn_cluster, umap_min_dist, umap_n_neighbours)\n",
    "    test_umap_embedding_nn_fov = generate_umap_embedding(X_test_nn_fov, umap_min_dist, umap_n_neighbours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"------ LOC ENCODER -------\")\n",
    "visualise_umap_embedding(train_umap_embedding_nn_loc, train_df_nn_loc, label_map)\n",
    "if final_test:\n",
    "    visualise_umap_embedding(test_umap_embedding_nn_loc, test_df_nn_loc, label_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"------ CLUSTER ENCODER -------\")\n",
    "visualise_umap_embedding(train_umap_embedding_nn_cluster, train_df_nn_cluster, label_map)\n",
    "if final_test:\n",
    "    visualise_umap_embedding(test_umap_embedding_nn_cluster, test_df_nn_cluster, label_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"------ FOV ENCODER -------\")\n",
    "visualise_umap_embedding(train_umap_embedding_nn_fov, train_df_nn_fov, label_map)\n",
    "if final_test:\n",
    "    visualise_umap_embedding(test_umap_embedding_nn_fov, test_df_nn_fov, label_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pca_embedding_nn_loc = generate_pca_embedding(X_train_nn_loc, pca_n_components)\n",
    "train_pca_embedding_nn_fov = generate_pca_embedding(X_train_nn_fov, pca_n_components)\n",
    "train_pca_embedding_nn_cluster = generate_pca_embedding(X_train_nn_cluster, pca_n_components)\n",
    "if final_test:\n",
    "    test_pca_embedding_nn_loc = generate_pca_embedding(X_test_nn_loc, pca_n_components)\n",
    "    test_pca_embedding_nn_fov = generate_pca_embedding(X_test_nn_fov, pca_n_components)\n",
    "    test_pca_embedding_nn_cluster = generate_pca_embedding(X_test_nn_cluster, pca_n_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"------ LOC ENCODER -------\")\n",
    "visualise_pca_embedding(train_pca_embedding_nn_loc, train_df_nn_loc, label_map)\n",
    "if final_test:\n",
    "    visualise_pca_embedding(test_pca_embedding_nn_loc, test_df_nn_loc, label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"------ CLUSTER ENCODER -------\")\n",
    "visualise_pca_embedding(train_pca_embedding_nn_cluster, train_df_nn_cluster, label_map)\n",
    "if final_test:\n",
    "    visualise_pca_embedding(test_pca_embedding_nn_cluster, test_df_nn_cluster, label_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"------ FOV ENCODER -------\")\n",
    "visualise_pca_embedding(train_pca_embedding_nn_fov, train_df_nn_fov, label_map)\n",
    "if final_test:\n",
    "    visualise_pca_embedding(test_pca_embedding_nn_fov, test_df_nn_fov, label_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"----- LOC ------\")\n",
    "k_means_fn(X_train_nn_loc, train_df_nn_loc, label_map)\n",
    "if final_test:\n",
    "    k_means_fn(X_test_nn_loc, test_df_nn_loc, label_map)\n",
    "\n",
    "print(\"----- CLUSTER ------\")\n",
    "k_means_fn(X_train_nn_cluster, train_df_nn_cluster, label_map)\n",
    "if final_test:\n",
    "    k_means_fn(X_test_nn_cluster, test_df_nn_cluster, label_map)\n",
    "\n",
    "print(\"----- FOV ------\")\n",
    "k_means_fn(X_train_nn_fov, train_df_nn_fov, label_map)\n",
    "if final_test:\n",
    "    k_means_fn(X_test_nn_fov, test_df_nn_fov, label_map)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph structure explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get item to evaluate on\n",
    "file_name = \"three_2296\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load in configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = os.path.join(project_directory, \"config/featanalyse_nn.yaml\")\n",
    "with open(config_path, \"r\") as ymlfile:\n",
    "    config = yaml.safe_load(ymlfile)\n",
    "\n",
    "# load in gt_label_map\n",
    "metadata_path = os.path.join(project_directory, \"metadata.json\")\n",
    "with open(\n",
    "    metadata_path,\n",
    ") as file:\n",
    "    metadata = json.load(file)\n",
    "    # add time ran this script to metadata\n",
    "    gt_label_map = metadata[\"gt_label_map\"]\n",
    "\n",
    "fold = config[\"fold\"]\n",
    "gt_label_map = {int(key): val for key, val in gt_label_map.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualise file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not final_test:\n",
    "    fold = config[\"fold\"]\n",
    "    file_folder = f\"processed/fold_{fold}\"\n",
    "else:\n",
    "    file_folder = \"processed\"\n",
    "visualise_file(project_directory, file_name, file_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load in cluster model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_model = torch.load(os.path.join(project_directory, f\"output/cluster_model.pt\"))\n",
    "cluster_model.to(device)\n",
    "cluster_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_train_folder = os.path.join(project_directory, \"processed/featanalysis/train\")\n",
    "cluster_val_folder = os.path.join(project_directory, \"processed/featanalysis/val\")\n",
    "cluster_test_folder = os.path.join(project_directory, \"processed/featanalysis/test\")\n",
    "\n",
    "if not final_test:\n",
    "    loc_test_folder = os.path.join(\n",
    "        project_directory, \"processed\", f\"fold_{fold}\", \"test\"\n",
    "    )\n",
    "    loc_train_folder = os.path.join(\n",
    "        project_directory, \"processed\", f\"fold_{fold}\", \"train\"\n",
    "    )\n",
    "    loc_val_folder = os.path.join(\n",
    "        project_directory, \"processed\", f\"fold_{fold}\", \"val\"\n",
    "    )\n",
    "\n",
    "else:\n",
    "    loc_test_folder = os.path.join(project_directory, \"processed\", \"test\")\n",
    "    loc_train_folder = os.path.join(project_directory, \"processed\", \"train\")\n",
    "    loc_val_folder = os.path.join(project_directory, \"processed\", \"val\")\n",
    "\n",
    "\n",
    "cluster_train_set = datastruc.ClusterDataset(\n",
    "    None,\n",
    "    cluster_train_folder,\n",
    "    label_level=None,\n",
    "    pre_filter=None,\n",
    "    save_on_gpu=None,\n",
    "    transform=None,\n",
    "    pre_transform=None,\n",
    "    fov_x=None,\n",
    "    fov_y=None,\n",
    ")\n",
    "\n",
    "cluster_val_set = datastruc.ClusterDataset(\n",
    "    None,\n",
    "    cluster_val_folder,\n",
    "    label_level=None,\n",
    "    pre_filter=None,\n",
    "    save_on_gpu=None,\n",
    "    transform=None,\n",
    "    pre_transform=None,\n",
    "    fov_x=None,\n",
    "    fov_y=None,\n",
    ")\n",
    "\n",
    "cluster_test_set = datastruc.ClusterDataset(\n",
    "    None,\n",
    "    cluster_test_folder,\n",
    "    label_level=None,\n",
    "    pre_filter=None,\n",
    "    save_on_gpu=None,\n",
    "    transform=None,\n",
    "    pre_transform=None,\n",
    "    fov_x=None,\n",
    "    fov_y=None,\n",
    ")\n",
    "\n",
    "# localisations are yet to be passed through any network\n",
    "\n",
    "loc_train_set = datastruc.ClusterLocDataset(\n",
    "    None,  # raw_loc_dir_root\n",
    "    None,  # raw_cluster_dir_root\n",
    "    loc_train_folder,  # processed_dir_root\n",
    "    label_level=None,\n",
    "    pre_filter=None,\n",
    "    save_on_gpu=None,\n",
    "    transform=None,\n",
    "    pre_transform=None,\n",
    "    loc_feat=None,\n",
    "    cluster_feat=None,\n",
    "    min_feat_locs=None,\n",
    "    max_feat_locs=None,\n",
    "    min_feat_clusters=None,\n",
    "    max_feat_clusters=None,\n",
    "    kneighboursclusters=None,\n",
    "    fov_x=None,\n",
    "    fov_y=None,\n",
    "    kneighbourslocs=None,\n",
    ")\n",
    "\n",
    "loc_val_set = datastruc.ClusterLocDataset(\n",
    "    None,  # raw_loc_dir_root\n",
    "    None,  # raw_cluster_dir_root\n",
    "    loc_val_folder,  # processed_dir_root\n",
    "    label_level=None,\n",
    "    pre_filter=None,\n",
    "    save_on_gpu=None,\n",
    "    transform=None,\n",
    "    pre_transform=None,\n",
    "    loc_feat=None,\n",
    "    cluster_feat=None,\n",
    "    min_feat_locs=None,\n",
    "    max_feat_locs=None,\n",
    "    min_feat_clusters=None,\n",
    "    max_feat_clusters=None,\n",
    "    kneighboursclusters=None,\n",
    "    fov_x=None,\n",
    "    fov_y=None,\n",
    "    kneighbourslocs=None,\n",
    ")\n",
    "\n",
    "loc_test_set = datastruc.ClusterLocDataset(\n",
    "    None,  # raw_loc_dir_root\n",
    "    None,  # raw_cluster_dir_root\n",
    "    loc_test_folder,  # processed_dir_root\n",
    "    label_level=None,\n",
    "    pre_filter=None,\n",
    "    save_on_gpu=None,\n",
    "    transform=None,\n",
    "    pre_transform=None,\n",
    "    loc_feat=None,\n",
    "    cluster_feat=None,\n",
    "    min_feat_locs=None,\n",
    "    max_feat_locs=None,\n",
    "    min_feat_clusters=None,\n",
    "    max_feat_clusters=None,\n",
    "    kneighboursclusters=None,\n",
    "    fov_x=None,\n",
    "    fov_y=None,\n",
    "    kneighbourslocs=None,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get items to evaluate on "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_dataitem, loc_dataitem, prediction = get_prediction(\n",
    "    file_name,\n",
    "    cluster_model, \n",
    "    cluster_train_set, \n",
    "    loc_train_set, \n",
    "    cluster_val_set, \n",
    "    loc_val_set, \n",
    "    cluster_test_set, \n",
    "    loc_test_set,\n",
    "    project_directory,\n",
    "    device, \n",
    "    gt_label_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SubgraphX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subgraph_config = {\n",
    "    # number of iterations to get prediction\n",
    "    \"rollout\":  10, # 20\n",
    "    # number of atoms of leaf node in search tree\n",
    "    \"min_atoms\": 5,\n",
    "    # hyperparameter that encourages exploration\n",
    "    \"c_puct\": 10.0,\n",
    "    # number of atoms to expand when extend the child nodes in the search tree\n",
    "    \"expand_atoms\": 14,\n",
    "    # whether to expand the children nodes from high degreee to low degree when extend the child nodes in the search tree\n",
    "    \"high2low\": False,\n",
    "    # number of local radius to caclulate\n",
    "    \"local_radius\": 4,\n",
    "    # sampling time of montecarlo approxim\n",
    "    \"sample_num\": 10, # 100\n",
    "    # reward method\n",
    "    \"reward_method\": \"mc_l_shapley\",\n",
    "    # subgrpah building method\n",
    "    \"subgraph_building_method\": \"split\",\n",
    "    # maximum number of nodes to include in subgraph when generating explanation\n",
    "    \"max_nodes\": 8,\n",
    "    # number of classes\n",
    "    \"num_classes\": 7,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # ---- subgraphx -----\n",
    "subgraph_eval(cluster_model, device, subgraph_config, cluster_dataitem, prediction)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PgEx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgex_config = {\n",
    "    # threshold to apply to edge mask for pyg explain\n",
    "    \"edge_mask_threshold\": 0.5,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- pgexplainer ----\n",
    "pg_explainer = torch.load(os.path.join(project_directory, f\"output/pg_explainer.pt\")) \n",
    "pgex_eval(pg_explainer, cluster_dataitem, device, pgex_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_config = {\n",
    "    #  scale: cluster # Options: [cluster] In future will also include [loc, loccluster]\n",
    "    \"scale\": \"cluster\",\n",
    "    #  # how to combine attention scores across multiple attention heads\n",
    "    \"reduce\": \"max\",\n",
    "    # threshold to apply to edge mask for pyg explain\n",
    "    \"edge_mask_threshold\": 0.5,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- attention -----\n",
    "# use model - logprobs or clustermodel - raw\n",
    "attention_eval(cluster_model, attention_config, cluster_dataitem, device, loc_dataitem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical tests"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
