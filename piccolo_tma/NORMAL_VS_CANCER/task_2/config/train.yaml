# device to train on (True: gpu False: cpu)
train_on_gpu: True

# load data directly from the gpu
load_data_from_gpu: False

# optimiser parameters
optimiser: adam # Options: [adam]
lr: 0.001 # Learning rate
weight_decay: 0.0001

# training parameters
epochs: 100
batch_size: 2
num_workers: 1 # generall higher -> faster
loss_fn: nll # Options: [nll]

# whether to sample imbalanced i.e. oversample from minority class/undersample from majority
imbalanced_sampler: True

# whether making node or whole graph predictions
label_level: graph # Options: [node, graph]

# train/val transforms
transforms:
  # how much to jitter points
  # 1 unit in new space ~= max(fov_x, fov_y)/2 - it will differ slightly as data itself might not cover full fov range
  # fov_x/fov_y is defined in process.yaml
  # here 1 unit = 40,000nm
  # therefore 15nm = 0.000375 units
  #jitter: 0.000375
  ## how much to scale the points
  #randscale: [0.95,1.05]
  ## how much to shear the points
  #shear: 0.05
  ## if key specified then randomly flipped in x/y axis
  ## (note value is irrelevant therefore have put as null)
  #x_flip: null
  #y_flip: null
  ## if key specified then randomly rotated around z axis
  ## (note value is irrelevant therefore have put as null)
  #z_rotate: null


model: locclusternettransformer
locclusternettransformer:
  # +++++ PointTransformer parameters +++++
  # Ratio of points to sample from the point cloud
  ratio: 1.0
  # For each point we sample nearest neighbours up to nearest neighbours (k)
  k: 4
  # Number of channels input to first MLP
  in_channels: 0
  # Number of channels out of the final layer
  out_channels: 8
  # Channels for the transformer/transition down blocks
  dim_model: [16, 32, 64]
  pos_nn_layers: 64
  attn_nn_layers: 64
  # Hidden channels for final MLP
  output_mlp_layers: 128

  # +++++ ClusterNet parameters +++++
  # Dropout for each layer
  dropout: 0.0
  conv_type: pointtransformer # Options: [gin, transformer, pointnet, pointtransformer]
  # number of final output channels
  OutputChannels: 2
  # add position coordinates to each cluster
  add_cluster_pos: False
  # --- if conv_type == gin OR pointnet ------------
  ClusterEncoderChannels: [[10, 16, 16],[18,20,24],[26,28,32]]
  # ------------------------------------
  # --- if conv_type == transformer ----
  # Out channels for each layer
  tr_out_channels: [16, 24, 32]
  # Number of multihead attention layers
  tr_heads: 1
  # If False multi-head attentions are averaged rather than being concatenated
  tr_concat: True
  # See https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.TransformerConv.html for difference
  # when beta is True
  tr_beta: False
  # ------------------------------------
  # --- if conv_type == pointtransformer ------------
  # input channel size for each layer
  pt_tr_in_channels: [8, 32, 64]
  # output channel size for each layer
  pt_tr_out_channels: [32, 64, 32]
  # size of hidden channel for position NN
  pt_tr_pos_nn_layers: 64
  # size of hidden channel for attention NN
  pt_tr_attn_nn_layers: 64
  # dimensions of data: options = [2,3]
  pt_tr_dim: 2
  # -------------------------------------------------
#
# model: clusternet
# clusternet:
#
#   # Dropout for each layer
#   dropout: 0.5
#
#   # +++++ ClusterNet parameters +++++
#   conv_type: gin # Options: [gin, transformer, pointnet, pointtransformer]
#   # number of final output channels
#   OutputChannels: 2
#   # add position coordinates to each cluster
#   add_cluster_pos: True
#
#   # --- if conv_type == gin OR pointnet ------------
#   ClusterEncoderChannels: [[10, 16, 16],[18,20,24],[26,28,32]]
#   # ------------------------------------
#
#   # --- if conv_type == transformer ----
#   # Out channels for each layer
#   tr_out_channels: [16, 24, 32]
#   # Number of multihead attention layers
#   tr_heads: 1
#   # If False multi-head attentions are averaged rather than being concatenated
#   tr_concat: True
#   # See https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.TransformerConv.html for difference
#   # when beta is True
#   tr_beta: False
#   # ------------------------------------
#
#   # --- if conv_type == pointtransformer ------------
#   # input channel size for each layer
#   pt_tr_in_channels: [16, 32, 64]
#   # output channel size for each layer
#   pt_tr_out_channels: [32, 64, 32]
#   # size of hidden channel for position NN
#   pt_tr_pos_nn_layers: 64
#   # size of hidden channel for attention NN
#   pt_tr_attn_nn_layers: 64
#   # dimensions of data: options = [2,3]
#   pt_tr_dim: 2
#   # -------------------------------------------------
#
# model: clustermlp
# clustermlp:
#   # Dropout for each layer
#   dropout: 0.5
#
#   # Channels for MLP applied to clusters
#   channels: [8, 16, 24, 32, 2]
#
# model: locnetonly_pointnet
# locnetonly_pointnet:
#   # +++++ PointNet parameters +++++
#   local_channels: [[2, 32, 32, 64],[130, 64, 64, 128]]
#   global_channels: [[64, 128, 128],[128, 256, 256]]
#   global_sa_channels: [258, 128, 128, 128]
#   final_channels: [128, 64, 32, 2]
#   # Ratio of points to sample from the point cloud
#   ratio: 1.0
#   # For each point we sample nearest neighbours up to radius and nearest neighbours (k)
#   radius: 1.0
#   k: 4
#
# model: locnetonly_pointtransformer
# locnetonly_pointtransformer:
#   # +++++ PointTransformer parameters +++++
#   # Ratio of points to sample from the point cloud
#   ratio: 1.0
#   # For each point we sample nearest neighbours up to nearest neighbours (k)
#   k: 4
#   # Number of channels input to first MLP
#   in_channels: 0
#   # Number of channels out of the final layer
#   out_channels: 2
#   # Channels for the transformer/transition down blocks
#   dim_model: [16, 32, 64]
#   pos_nn_layers: 64
#   attn_nn_layers: 64
#   # Hidden channels for final MLP
#   output_mlp_layers: 128
