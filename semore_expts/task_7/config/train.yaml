# device to train on (gpu or cpu)
train_on_gpu: True
load_data_from_gpu: False

# model parameters
model: locclusternet

# parameters for the network
locclusternet:

  # +++ LocNet parameters +++
  loc_conv_type: pointnet # Options: [pointnet, pointtransformer]
  # Ratio of points to sample from the point cloud
  ratio: 0.5
  # For each point we sample nearest neighbours up to nearest neighbours (k)
  k: 5

  # +++++ PointNet parameters +++++
  local_channels: [[2, 4, 6],[12, 14, 16], [22,24,26]]
  global_channels: [[6, 8, 10],[16, 18, 20], [26,28,30]]
  global_sa_channels: [32, 30, 28, 26]
  final_channels: [26, 24, 22, 8]
  # For each point we sample nearest neighbours up to radius
  radius: 1.0

  # +++++ PointTransformer parameters +++++
  # Number of channels input to first MLP
  in_channels: 0
  # Number of channels out of the final layer
  out_channels: 8
  # Channels for the transformer/transition down blocks
  dim_model: [16, 32, 64]
  pos_nn_layers: 64
  attn_nn_layers: 64
  # Hidden channels for final MLP
  output_mlp_layers: 128

  # ++++++++++++++++++++++++++++++++++++++++

  # +++++ ClusterNet parameters +++++
  # Dropout for each layer
  dropout: 0.0
  cluster_conv_type: pointnet # Options: [gin, transformer, pointnet, pointtransformer]
  # number of final output channels
  OutputChannels: 7
  # add position coordinates to each cluster
  add_cluster_pos: False

  # --- if cluster_conv_type == gin OR pointnet ------------
  ClusterEncoderChannels: [[10, 12, 16],[18, 20, 24],[26, 28, 32],[34, 36, 40]]
  # ------------------------------------

  # --- if cluster_conv_type == transformer ----
  # Out channels for each layer
  tr_out_channels: [16, 24, 32, 40]
  # Number of multihead attention layers
  tr_heads: 1
  # If False multi-head attentions are averaged rather than being concatenated
  tr_concat: True
  # See https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.TransformerConv.html for difference
  # when beta is True
  tr_beta: False
  # ------------------------------------

  # --- if cluster_conv_type == pointtransformer ------------
  # input channel size for each layer
  pt_tr_in_channels: [8, 16, 24, 32]
  # output channel size for each layer
  pt_tr_out_channels: [16, 24, 32, 40]
  # size of hidden channel for position NN
  pt_tr_pos_nn_layers: 32
  # size of hidden channel for attention NN
  pt_tr_attn_nn_layers: 32
  # dimensions of data: options = [2,3]
  pt_tr_dim: 2
  # -------------------------------------------------


# optimiser parameters
optimiser: adam
lr: 0.001
weight_decay: 0.0001

# training parameters
epochs: 50
batch_size: 2
num_workers: 1 # generall higher -> faster
loss_fn: nll

# whether to sample imbalanced i.e. oversample from minority class/undersample from majority
imbalanced_sampler: False

# what trying to predict
label_level: graph # node

# train/val transforms
transforms:
  # 1 unit in new space ~= max(fov_x, fov_y)/2 - it will differ slightly as data itself might not cover full fov range
  # fov_x/fov_y is defined in process.yaml
  # here 1 unit = 20,000nm
  # therefore 15nm = 0.00075 units
  jitter: 0.00075 # number of nm
  randscale: [0.95,1.05]
  shear: 0.05
  x_flip: null
  y_flip: null
  z_rotate: null

# pointnetclass:
#   ratio : [0.5,0.25]
#   radius : [0.2,0.4]
#   # 130 = 128 + dim (2) + dim of feature (0)
#   # 258 = 256 + dim (2) + dim of feature (0)
#   # num classes = 2
#   channels : [[3, 64, 64, 128], [130, 128, 128, 256], [258, 256, 512, 1024], [1024, 512, 256, 2]]
#   dropout : 0.5
#   norm : null
#
# pointnetseg:
#   ratio : [0.2,0.25]
#   radius : [0.2, 0.4]
#   # 2 = dim (2) + dim of feature (0)
#   # 130 = 128 + dim (2) + dim of feature (0)
#   # 258 = 256 + dim (2) + dim of feature (0)
#   sa_channels : [[2, 64, 64, 128], [130, 128, 128, 256], [258, 256, 512, 1024]]
#   k : [1,3,3]
#   # 1080 = 1024 + 256
#   # 384 = 256 + 128
#   # 130 = 128 + dim (2) + dim of feature (0)
#   fp_channels : [[1080, 256, 256], [384, 256, 128], [130, 128, 128, 128]]
#   # num classes = 2
#   output_channels : [128, 128, 128, 2]
#   dropout : 0.5
#   norm : null
#
# pointtransformerclass:
#   # used in both transition down and for considreing how many neighbours point transformer should consider
#   k: 16
#   in_channels: 2
#   out_channels: 2
#   dim_model: [32, 64, 128, 256, 512]
#   output_mlp_layers: 64
#   # ratio of points to sample when transition down
#   ratio: 0.25
#   pos_nn_layers: 64
#   attn_nn_layers: 64
#
# pointtransformerseg:
#   # used in both transition down and for considreing how many neighbours point transformer should consider
#   k: 16
#   in_channels: 2
#   out_channels: 2
#   dim_model: [32, 64, 128, 256, 512]
#   k_up: 3 # trilinear interpolation
#   output_mlp_layers: 64
#   # ratio of points to sample when transition down
#   ratio: 0.25
#   pos_nn_layers: 64
#   attn_nn_layers: 64
