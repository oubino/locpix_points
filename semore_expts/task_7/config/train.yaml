# device to train on (gpu or cpu)
train_on_gpu: True
load_data_from_gpu: False

# model parameters
model: locnetclassifyfov

# parameters for the network
locnetclassifyfov:
  local_channels: [[2,64],[66,128]]
  global_channels: [[64,64],[128,128]]
  global_sa_channels: [130, 256]
  final_channels: [256, 2]
  dropout: 0.5
  norm: batch_norm

# optimiser parameters
optimiser: adam
lr: 0.001
weight_decay: 0.001

# training parameters
epochs: 100
batch_size: 2
num_workers: 1 # generall higher -> faster
loss_fn: nll

# what trying to predict
label_level: graph # node

# train/val transforms
transforms:
  # 1 unit in new space ~= max(fov_x, fov_y)/2 - it will differ slightly as data itself might not cover full fov range
  # fov_x/fov_y is defined in process.yaml
  # here 1 unit = 20,000nm
  # therefore 15nm = 0.00075 units
  # therefore 30nm = 0.00150 units
  jitter: 0.00075 # number of nm
  randscale: [0.95,1.05]
  shear: 0.05 # 05
  x_flip: null
  y_flip: null
  z_rotate: null

# pointnetclass:
#   ratio : [0.5,0.25]
#   radius : [0.2,0.4]
#   # 130 = 128 + dim (2) + dim of feature (0)
#   # 258 = 256 + dim (2) + dim of feature (0)
#   # num classes = 2
#   channels : [[3, 64, 64, 128], [130, 128, 128, 256], [258, 256, 512, 1024], [1024, 512, 256, 2]]
#   dropout : 0.5
#   norm : null
#
# pointnetseg:
#   ratio : [0.2,0.25]
#   radius : [0.2, 0.4]
#   # 2 = dim (2) + dim of feature (0)
#   # 130 = 128 + dim (2) + dim of feature (0)
#   # 258 = 256 + dim (2) + dim of feature (0)
#   sa_channels : [[2, 64, 64, 128], [130, 128, 128, 256], [258, 256, 512, 1024]]
#   k : [1,3,3]
#   # 1080 = 1024 + 256
#   # 384 = 256 + 128
#   # 130 = 128 + dim (2) + dim of feature (0)
#   fp_channels : [[1080, 256, 256], [384, 256, 128], [130, 128, 128, 128]]
#   # num classes = 2
#   output_channels : [128, 128, 128, 2]
#   dropout : 0.5
#   norm : null
#
# pointtransformerclass:
#   # used in both transition down and for considreing how many neighbours point transformer should consider
#   k: 16
#   in_channels: 2
#   out_channels: 2
#   dim_model: [32, 64, 128, 256, 512]
#   output_mlp_layers: 64
#   # ratio of points to sample when transition down
#   ratio: 0.25
#   pos_nn_layers: 64
#   attn_nn_layers: 64
#
# pointtransformerseg:
#   # used in both transition down and for considreing how many neighbours point transformer should consider
#   k: 16
#   in_channels: 2
#   out_channels: 2
#   dim_model: [32, 64, 128, 256, 512]
#   k_up: 3 # trilinear interpolation
#   output_mlp_layers: 64
#   # ratio of points to sample when transition down
#   ratio: 0.25
#   pos_nn_layers: 64
#   attn_nn_layers: 64
